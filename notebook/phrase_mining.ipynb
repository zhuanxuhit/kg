{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çŸ­è¯­æŒ–æ˜æ–¹æ³•ä»‹ç»\n",
    "\n",
    "ç¬¬ä¸€å¤§ç±»ï¼šUnigram topic modeling ä¸€å…ƒçš„ä¸»é¢˜æ¨¡å‹ï¼Œä¸»è¦åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. Latent Semantic Analysis (LSA)\n",
    "2. Probablistic Latent Semantic Analysis ï¼ˆpLSAï¼‰\n",
    "3. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "ä¸‹é¢å¼€å§‹å¯¹è¿™å‡ ç§æ¨¡å‹çš„ä»‹ç»\n",
    "\n",
    "### 1.  LSA\n",
    "å‚è€ƒåšæ–‡[å…³äº LSAï¼ˆLatent Semantic Analysisï¼‰ä¸»é¢˜æ¨¡å‹çš„ä¸ªäººç†è§£](http://blog.csdn.net/cang_sheng_ta_ge/article/details/46708515)ï¼Œå¦å¤–ä¸€ç¯‡è‹±æ–‡æ–‡ç« [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "\n",
    "å…¶ä¸»è¦æ€æƒ³æ„Ÿè§‰æœ‰ç‚¹ç±»ä¼¼äºè¯åµŒå…¥ï¼Œæˆ‘ä»¬å‡è®¾æœ‰å¥½å¤šè¯ï¼Œé‚£å¯ä»¥å°†æ–‡æ¡£ç”¨è¯æ¥è¿›è¡Œone-hot encodingï¼Œä½†æ˜¯è¿™æ ·ä¼šé€ æˆæ–‡ç« å¤§é‡çš„ç¨€ç–è¡¨ç¤ºï¼Œäºæ˜¯æˆ‘ä»¬æƒ³æ³•å°±æ˜¯æ€ä¹ˆèƒ½å¤Ÿå°†ç¨€ç–çŸ©é˜µå˜ä¸ºç¨ å¯†çŸ©é˜µï¼Œä¸€ä¸ªåšæ³•å°±æ˜¯çŸ©é˜µåˆ†è§£ã€‚\n",
    "\n",
    "ä¸‹é¢ä»£ç å‚è€ƒ https://github.com/chrisjmccormick/LSA_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„çš„æ•°æ®ä½¿ç”¨çš„æ˜¯è·¯é€ç¤¾çš„æ•°æ®ï¼Œå¤§æ¦‚ç±»åˆ«æœ‰100å·¦å³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n"
     ]
    }
   ],
   "source": [
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¡ç®— LSA ç¬¬ä¸€æ­¥å°±æ˜¯å°±æŒ‰tf-idfå€¼ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ sklearn ä¸­çš„æ¨¡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬å»é™¤äº†åœç”¨è¯\n",
    "# è¿‡æ»¤æ‰å‡ºç°æ–‡æ¡£è¶…è¿‡50%çš„è¯\n",
    "# è¿‡æ»¤æ‰æœ€å°å‡ºç°2æ¬¡çš„è¯\n",
    "# æŒ‘é€‰å‡ºå‰1wä¸ªè¯\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: 10000\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4743, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹ä¸€æ­¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯å¯¹é«˜ç»´çš„ç‰¹å¾çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼ŒSVDå¥‡å¼‚å€¼åˆ†è§£ã€‚\n",
    "\n",
    "X_train_tfidf ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ç¯‡æ–‡ç« ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªè¯ï¼Œæ¯ä¸ªæ•°å€¼éƒ½æ˜¯tf-idfå€¼ã€‚\n",
    "\n",
    "å¥‡å¼‚å€¼åˆ†è§£åšçš„äº‹æƒ…å°±æ˜¯å°†é«˜çº¬åº¦ç¨€ç–çŸ©é˜µåˆ†è§£ä¸º3ä¸ªçŸ©é˜µè¿ä¹˜ã€‚ç½‘ä¸Šæ‰¾åˆ°ä¸€ç¯‡éå¸¸å¥½çš„æ–‡ç« ï¼š[å¥‡å¼‚å€¼åˆ†è§£ (SVD) --- å‡ ä½•æ„ä¹‰](http://blog.sciencenet.cn/blog-696950-699432.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å…ˆå¯¹è®­ç»ƒæ•°æ®æ±‚å–svdï¼Œç„¶åå¯¹é½è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¾—åˆ°å•ä½å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾—åˆ°è½¬æ¢åçš„ç‰¹å¾å€¼ï¼Œå¹¶ä¸”è¿™äº›ç‰¹å¾å æ€»çš„ä¿¡æ¯é‡ä¸º27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Explained variance of the SVD step: 27%\n"
     ]
    }
   ],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢å¼€å§‹å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4471 / 4858) correct - 92.03%\n",
      "  done in 1.404sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4561 / 4858) correct - 93.89%\n",
      "    done in 0.749sec\n"
     ]
    }
   ],
   "source": [
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(X_test_lsa)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensimä½¿ç”¨\n",
    "ä½¿ç”¨ gensim æ¥åš LSA çš„åˆ†æï¼ŒLatent Semantic Indexing (also known as Latent Semantic Analysis)ï¼Œåœ¨ gensim ä¸­å¯¹åº”çš„æ¨¡å—æ˜¯models.lsimodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTfidfCorups():\n",
    "    import pickle\n",
    "    raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "    X_train_raw = raw_text_dataset[0]\n",
    "    y_train_labels = raw_text_dataset[1] \n",
    "    X_test_raw = raw_text_dataset[2]\n",
    "    y_test_labels = raw_text_dataset[3]\n",
    "    \n",
    "    # Tokenize the documents.\n",
    "\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "    def docTokenizer(docs):\n",
    "        # Split the documents into tokens.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        newdocs = []\n",
    "        for idx in range(len(docs)):\n",
    "            doc = docs[idx].lower()  # Convert to lowercase.\n",
    "            newdocs.append(tokenizer.tokenize(doc))  # Split into words.\n",
    "\n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        newdocs = [[token for token in doc if not token.isnumeric()] for doc in newdocs]\n",
    "\n",
    "        # Remove words that are only one character.\n",
    "        newdocs = [[token for token in doc if len(token) > 1] for doc in newdocs]\n",
    "\n",
    "        return newdocs\n",
    "    \n",
    "    x_train_tokens = docTokenizer(X_train_raw)\n",
    "    x_test_tokens = docTokenizer(X_test_raw)\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    x_train_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_train_tokens]\n",
    "    x_test_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_test_tokens]\n",
    "    \n",
    "    # Remove rare and common tokens.\n",
    "    import numpy as np\n",
    "    from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(np.concatenate((x_train_tokens,x_test_tokens)))\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "    \n",
    "    from gensim.models import TfidfModel\n",
    "    # Vectorize data.\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corups = [dictionary.doc2bow(doc) for doc in np.concatenate((x_train_tokens,x_test_tokens))]\n",
    "    \n",
    "    corpus_tfidf = tfidf[corups]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "æ•´ä¸ªå¤„ç†è¿‡ç¨‹å¯ä»¥åˆ†ä¸º4æ­¥ï¼š\n",
    "\n",
    "1. å°†æ–‡æ¡£åˆ†è¯\n",
    "2. å•è¯Lemmatize\n",
    "3. idfå€¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docTokenizer(docs):\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    newdocs = []\n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx].lower()  # Convert to lowercase.\n",
    "        newdocs.append(tokenizer.tokenize(doc))  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    newdocs = [[token for token in doc if not token.isnumeric()] for doc in newdocs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    newdocs = [[token for token in doc if len(token) > 1] for doc in newdocs]\n",
    "    \n",
    "    return newdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_tokens = docTokenizer(X_train_raw)\n",
    "x_test_tokens = docTokenizer(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "x_train_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_train_tokens]\n",
    "x_test_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹ä¸€æ­¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯å»ºç«‹corupsäº†ï¼Œå»é™¤è¶…è¿‡åœ¨ä¸€åŠæ–‡æ¡£éƒ½å‡ºç°çš„è¯ï¼Œå»é™¤å‡ºç°æ–‡æ¡£æ•°å°äº20çš„è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:24:20,453 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-09-30 16:24:22,217 : INFO : built Dictionary(27466 unique tokens: ['bahia', 'cocoa', 'review', 'shower', 'continued']...) from 9601 documents (total 1214137 corpus positions)\n",
      "2017-09-30 16:24:22,279 : INFO : discarding 24027 tokens: [('bahia', 1), ('shower', 6), ('the', 7371), ('in', 6381), ('alleviating', 2), ('and', 6869), ('for', 5439), ('temporao', 2), ('humidity', 3), ('restored', 18)]...\n",
      "2017-09-30 16:24:22,281 : INFO : keeping 3439 tokens which were in no less than 20 and no more than 4800 (=50.0%) documents\n",
      "2017-09-30 16:24:22,293 : INFO : resulting dictionary: Dictionary(3439 unique tokens: ['cocoa', 'review', 'continued', 'throughout', 'week']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(np.concatenate((x_train_tokens,x_test_tokens)))\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„å»é™¤å®Œåï¼Œå­—å…¸å°±å‰© 3439 äº†ï¼Œä¸‹é¢æˆ‘ä»¬å¤„ç†è¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "# Vectorize data.\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "# Bag-of-words representation of the documents.\n",
    "corups = [dictionary.doc2bow(doc) for doc in np.concatenate((x_train_tokens,x_test_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel,TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:42:37,165 : INFO : using serial LSI version on this node\n",
      "2017-09-30 16:42:37,167 : INFO : updating model with new documents\n",
      "2017-09-30 16:42:38,488 : INFO : preparing a new chunk of documents\n",
      "2017-09-30 16:42:38,674 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-09-30 16:42:38,676 : INFO : 1st phase: constructing (3439, 200) action matrix\n",
      "2017-09-30 16:42:38,922 : INFO : orthonormalizing (3439, 200) action matrix\n",
      "2017-09-30 16:42:39,723 : INFO : 2nd phase: running dense svd on (200, 9601) matrix\n",
      "2017-09-30 16:42:39,900 : INFO : computing the final decomposition\n",
      "2017-09-30 16:42:39,903 : INFO : keeping 100 factors (discarding 20.809% of energy spectrum)\n",
      "2017-09-30 16:42:39,909 : INFO : processed documents up to #9601\n",
      "2017-09-30 16:42:39,912 : INFO : topic #0(24.459): -0.640*\"v\" + -0.364*\"ct\" + -0.289*\"net\" + -0.261*\"loss\" + -0.228*\"shr\" + -0.196*\"mln\" + -0.173*\"rev\" + -0.155*\"profit\" + -0.142*\"qtr\" + -0.098*\"oper\"\n",
      "2017-09-30 16:42:39,914 : INFO : topic #1(16.559): 0.180*\"pct\" + -0.173*\"v\" + 0.163*\"bank\" + 0.140*\"share\" + 0.136*\"billion\" + 0.122*\"dlrs\" + 0.113*\"will\" + 0.110*\"he\" + 0.110*\"is\" + 0.108*\"be\"\n",
      "2017-09-30 16:42:39,916 : INFO : topic #2(13.214): -0.386*\"qtly\" + -0.366*\"div\" + -0.330*\"ct\" + -0.301*\"record\" + -0.268*\"prior\" + -0.262*\"pay\" + -0.255*\"april\" + 0.217*\"loss\" + -0.185*\"dividend\" + -0.180*\"quarterly\"\n",
      "2017-09-30 16:42:39,919 : INFO : topic #3(9.648): 0.368*\"share\" + -0.217*\"billion\" + 0.212*\"offering\" + 0.195*\"stock\" + 0.182*\"common\" + 0.151*\"inc\" + -0.150*\"bank\" + 0.129*\"debenture\" + 0.123*\"convertible\" + 0.122*\"company\"\n",
      "2017-09-30 16:42:39,921 : INFO : topic #4(9.361): -0.794*\"loss\" + -0.294*\"profit\" + 0.244*\"v\" + 0.176*\"mln\" + 0.165*\"net\" + 0.091*\"billion\" + -0.086*\"oper\" + -0.083*\"qtly\" + 0.082*\"pct\" + 0.077*\"shr\"\n"
     ]
    }
   ],
   "source": [
    "lsi = LsiModel(corpus_tfidf, num_topics=100,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [[item[1] for item in vec] for vec in  lsi[corpus_tfidf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858 9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw),len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4593 / 4858) correct - 94.55%\n",
      "  done in 0.699sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(vectors[:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(vectors[4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ä¸Šé¢çš„é¢„å¤„ç†ï¼Œæˆ‘ä»¬æ¨¡å‹çš„é¢„æµ‹ç»“æœåˆæœ‰äº†æå‡ ğŸ˜‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA çš„å»¶ä¼¸\n",
    "çœ‹åˆ°ä¸Šé¢ LSA æ–¹æ³•çš„æ—¶å€™ï¼Œå…¶å®æˆ‘å°±æƒ³åˆ°äº† word embedingï¼Œé‚£æˆ‘ä»¬èƒ½å¤Ÿç”¨ç¥ç»ç½‘ç»œæ¥åšLSAå˜›ï¼Œæ€è·¯ï¼šæˆ‘ä»¬å°†documentè¿›è¡Œembedingï¼Œç„¶åå°†è¯ä¹Ÿè¿›è¡Œembeddingï¼Œç„¶åç»è¿‡ä¸€ä¸ª3å±‚å…¨è¿æ¥ï¼Œè¾“å‡ºè¯åœ¨documentä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œæˆ‘ä»¬ä¸æ–­çš„æ‹Ÿåˆä¸Šé¢çš„ç½‘ç»œï¼Œå°±èƒ½å¾—åˆ° document å’Œ word çš„å‘é‡äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw)+len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInput = keras.Input((1,),name='word_input',dtype='int32')\n",
    "docInput = keras.Input((1,),name='doc_input',dtype='int32')\n",
    "# countInput = keras.Input((1,),name='count_input',dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_topic_num = 100\n",
    "wordEmbed = keras.layers.embeddings.Embedding(10000,latent_topic_num,input_length=1,\n",
    "                                              embeddings_initializer=keras.initializers.VarianceScaling(2),name='word_embed')\n",
    "docEmbed = keras.layers.embeddings.Embedding(9601,latent_topic_num,input_length=1,\n",
    "                                  embeddings_initializer=keras.initializers.VarianceScaling(2),name='doc_embed')\n",
    "word_embed = keras.layers.Reshape((latent_topic_num,))(wordEmbed(wordInput)) # can also use Flatten\n",
    "doc_embed = keras.layers.Reshape((latent_topic_num,))(docEmbed(docInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_16/Reshape:0' shape=(?, 100) dtype=float32>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vector = keras.layers.concatenate([word_embed,doc_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_9/concat:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_len = 3\n",
    "# layer_num = 10\n",
    "layer_nums = [20,10,5]\n",
    "for i in range(len(layer_nums)):\n",
    "    input_vector = keras.layers.Dense(layer_nums[i],\n",
    "                                      kernel_initializer=keras.initializers.VarianceScaling(2),\n",
    "                                     activation='relu')(input_vector)\n",
    "    \n",
    "y_predict = keras.layers.Dense(1,kernel_initializer=keras.initializers.VarianceScaling(2))(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model([wordInput,docInput],y_predict)\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢æ ¹æ®ä¸Šé¢çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ¥å®šåˆ¶æˆ‘ä»¬çš„è¾“å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_vectorizer = CountVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_documents = np.concatenate((X_train_raw,X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_tf = dnn_vectorizer.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 10000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_frequence = doc_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInputs = []\n",
    "docInputs = []\n",
    "targetCounts = []\n",
    "for i in range(len(term_frequence)):\n",
    "    doc_frequence = term_frequence[i]\n",
    "    for j in range(len(doc_frequence)):\n",
    "        if doc_frequence[j] != 0:\n",
    "            docInputs.append(i)\n",
    "            wordInputs.append(j)\n",
    "            targetCounts.append(doc_frequence[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "doc_input (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word_embed (Embedding)           (None, 1, 5)          48005       word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "doc_embed (Embedding)            (None, 1, 5)          50000       doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)             (None, 5)             0           word_embed[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)             (None, 5)             0           doc_embed[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 10)            0           reshape_12[0][0]                 \n",
      "                                                                   reshape_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 20)            220         concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 10)            210         dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 5)             55          dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 1)             6           dense_26[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 98,496\n",
      "Trainable params: 98,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498923"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3101     \n",
      "Epoch 2/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3077     \n",
      "Epoch 3/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3062     \n",
      "Epoch 4/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3030     \n",
      "Epoch 5/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3010     \n",
      "Epoch 6/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2990     \n",
      "Epoch 7/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2963     \n",
      "Epoch 8/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2961     \n",
      "Epoch 9/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2949     \n",
      "Epoch 10/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2915     \n",
      "Epoch 11/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2895     \n",
      "Epoch 12/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2881     \n",
      "Epoch 13/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2870     \n",
      "Epoch 14/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2853     \n",
      "Epoch 15/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2831     \n",
      "Epoch 16/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2817     \n",
      "Epoch 17/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2801     \n",
      "Epoch 18/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2795     \n",
      "Epoch 19/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2769     \n",
      "Epoch 20/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2756     \n",
      "Epoch 21/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2743     \n",
      "Epoch 22/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2731     \n",
      "Epoch 23/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2715     \n",
      "Epoch 24/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2701     \n",
      "Epoch 25/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2692     \n",
      "Epoch 26/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2668     \n",
      "Epoch 27/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2654     \n",
      "Epoch 28/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2647     \n",
      "Epoch 29/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2639     \n",
      "Epoch 30/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2622     \n",
      "Epoch 31/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2613     \n",
      "Epoch 32/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2605     \n",
      "Epoch 33/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2592     \n",
      "Epoch 34/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2590     \n",
      "Epoch 35/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2578     \n",
      "Epoch 36/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2568     \n",
      "Epoch 37/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2574     \n",
      "Epoch 38/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2551     \n",
      "Epoch 39/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2530     \n",
      "Epoch 40/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2516     \n",
      "Epoch 41/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2502     \n",
      "Epoch 42/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2507     \n",
      "Epoch 43/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2484     \n",
      "Epoch 44/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2472     \n",
      "Epoch 45/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2469     \n",
      "Epoch 46/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2463     \n",
      "Epoch 47/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2448     \n",
      "Epoch 48/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2433     \n",
      "Epoch 49/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2420     \n",
      "Epoch 50/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2412     \n",
      "Epoch 51/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2418     \n",
      "Epoch 52/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2407     \n",
      "Epoch 53/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2402     \n",
      "Epoch 54/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2400     \n",
      "Epoch 55/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2397     \n",
      "Epoch 56/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2384     \n",
      "Epoch 57/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2368     \n",
      "Epoch 58/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2353     \n",
      "Epoch 59/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2346     \n",
      "Epoch 60/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2342     \n",
      "Epoch 61/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2339     \n",
      "Epoch 62/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2317     \n",
      "Epoch 63/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2322     \n",
      "Epoch 64/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2314     \n",
      "Epoch 65/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2307     \n",
      "Epoch 66/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2303     \n",
      "Epoch 67/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2289     \n",
      "Epoch 68/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2286     \n",
      "Epoch 69/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2277     \n",
      "Epoch 70/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2263     \n",
      "Epoch 71/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2261     \n",
      "Epoch 72/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2256     \n",
      "Epoch 73/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2254     \n",
      "Epoch 74/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2239     \n",
      "Epoch 75/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2241     \n",
      "Epoch 76/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2231     \n",
      "Epoch 77/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2224     \n",
      "Epoch 78/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2212     \n",
      "Epoch 79/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2199     \n",
      "Epoch 80/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2204     \n",
      "Epoch 81/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2207     \n",
      "Epoch 82/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2190     \n",
      "Epoch 83/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2185     \n",
      "Epoch 84/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2184     \n",
      "Epoch 85/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2177     \n",
      "Epoch 86/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2169     \n",
      "Epoch 87/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2163     \n",
      "Epoch 88/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2167     \n",
      "Epoch 89/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2155     \n",
      "Epoch 90/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2152     \n",
      "Epoch 91/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2145     \n",
      "Epoch 92/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2148     \n",
      "Epoch 93/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2139     \n",
      "Epoch 94/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2133     \n",
      "Epoch 95/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2128     \n",
      "Epoch 96/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2130     \n",
      "Epoch 97/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2118     \n",
      "Epoch 98/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2117     \n",
      "Epoch 99/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2106     \n",
      "Epoch 100/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2095     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d1079ef28>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array(wordInputs),np.array(docInputs)],np.array(targetCounts),batch_size=10000,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 5)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿›è¡Œåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4188 / 4858) correct - 86.21%\n",
      "    done in 0.515sec\n"
     ]
    }
   ],
   "source": [
    "#  (4183 / 4858) correct - 86.11%,\n",
    "#  0.4882 - 85.78% \n",
    "# 100ä¸ªhiddenï¼Œ100æ¬¡ï¼Œloss:0.3117 , correct - 86.39%\n",
    "# 100ä¸ªhiddenï¼Œ100æ¬¡ï¼Œloss:0.2095 , correct - 86.21%\n",
    "# å¯ä»¥è¯´ä¸Šé¢è¿™ç§åšæ³•\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(model.get_weights()[1][:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(model.get_weights()[1][4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»“è®ºï¼šé€šè¿‡ä¸Šé¢çš„dnnæ–¹æ³•ï¼Œbaselineçš„modelæ•ˆæœå¹¶ä¸æ˜¯å¾ˆå¥½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pLSA\n",
    "åœ¨LSAä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†termå’Œdocumentçš„å…±ç°çŸ©é˜µï¼Œ\n",
    "![å›¾ç‰‡](http://bos.nj.bpc.baidu.com/v1/agroup/afed8760421ed8b2b3d003e9b14f7f56673cc4a3)\n",
    "è¿™ä¸ªçš„æ ¸å¿ƒå‡è®¾æ˜¯ï¼šä½¿ç”¨è¯è¢‹æ¨¡å‹æ¥è¡¨ç¤ºæ–‡ç« ï¼ˆå³ä½¿ä¸¢å¤±äº†termçš„é¡ºåºï¼‰ä¹Ÿèƒ½å¾ˆå¥½çš„ä»£è¡¨æ–‡ç« ä¿¡æ¯ï¼ŒLSA é€šè¿‡svgæ–¹æ³•ï¼Œå°†documentæ˜ å°„åˆ°ä¸€ä¸ªéšå‘é‡ç©ºé—´ä¸Šå»ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ›´è¿›ä¸€æ­¥ï¼Œè€ƒè™‘å¼•å…¥ä¸€ä¸ªéšå«å˜é‡$z_k$ï¼Œæ¥è¡¨ç¤ºæ–‡æ¡£ä¸»é¢˜ï¼Œå½¢æˆç®€å•çš„è´å¶æ–¯ç½‘ç»œï¼Œ\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/f67618811933d48654161ae53fe5f2d836ace8d8)\n",
    "\n",
    "éå¸¸å¥½çš„æ–‡æ¡£å¯ä»¥å‚è€ƒï¼š\n",
    "\n",
    "[æ¦‚ç‡è¯­è¨€æ¨¡å‹åŠå…¶å˜å½¢ç³»åˆ— (1)-PLSA åŠ EM ç®—æ³•](http://blog.csdn.net/yangliuy/article/details/8330640)\n",
    "\n",
    "[è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹ PLSA](http://zhikaizhang.cn/2016/06/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8BPLSA/)\n",
    "\n",
    "[ä¸»é¢˜æ¨¡å‹ä¹‹ pLSA](http://blog.jqian.net/post/plsa.html)\n",
    "\n",
    "é‡Œé¢é‡è¦çš„ç‚¹æ˜¯EMçš„æ€æƒ³ï¼Œå³å…ˆå‡è®¾å‚æ•°$\\theta$å·²çŸ¥ï¼Œç„¶åæ±‚éšå˜é‡çš„åéªŒæ¦‚ç‡ï¼Œç„¶åæ­¤æ—¶çŸ¥é“éšå˜é‡åï¼Œå°±å¯ä»¥å’Œæ•°æ®Xä¸€èµ·ç»„æˆå®Œæ•´çš„æ•°æ®é›†ï¼Œæ­¤æ—¶å†æ¥æ±‚æå€¼ã€‚\n",
    "\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/4a83a5cf5beaced4dbe535759365c1bab4b2862b)\n",
    "\n",
    "ä¸‹é¢æ˜¯å…·ä½“çš„ç®—æ³•å®ç°ï¼Œä»£ç å‚è€ƒè‡ªï¼šhttps://github.com/laserwave/PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import sys\n",
    "import jieba\n",
    "import re\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ä»£ç ä¸­ä½¿ç”¨ $lamda[i,k]$ è¡¨ç¤ºå‚æ•° $p(z_k|d_i)$, ç”¨ $theta[k,j]$ è¡¨ç¤ºå‚æ•° $p(w_j|z_k)$, ç”¨ $p[i,j,k]$ è¡¨ç¤ºéšè—å˜é‡çš„åéªŒæ¦‚ç‡ $p(z_k|d_i,w_j)$ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–å‚æ•° $p(z_k|d_i)$, $p(w_j|z_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeParameters():\n",
    "    for i in range(0, N):\n",
    "        normalization = sum(lamda[i, :])\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] /= normalization\n",
    "\n",
    "    for k in range(0, K):\n",
    "        normalization = sum(theta[k, :])\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬äºŒæ­¥æ˜¯å¯¹äºæ–‡æ¡£æ•°æ®çš„å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦å¾—åˆ°ç»Ÿè®¡æ•°æ®ï¼Œ$n(d_i),n(d_i,w_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segmentation, stopwords filtering and document-word matrix generating\n",
    "# [return]:\n",
    "# N : number of documents\n",
    "# M : length of dictionary\n",
    "# word2id : a map mapping terms to their corresponding ids\n",
    "# id2word : a map mapping ids to terms\n",
    "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
    "def preprocessing(datasetFilePath, stopwordsFilePath):\n",
    "    \n",
    "    # read the stopwords file\n",
    "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
    "    stopwords = [line.strip() for line in file] \n",
    "    file.close()\n",
    "    \n",
    "    # read the documents\n",
    "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
    "    documents = [document.strip() for document in file] \n",
    "    file.close()\n",
    "\n",
    "    # number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    wordCounts = [];\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    currentId = 0;\n",
    "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
    "    for document in documents:\n",
    "        segList = jieba.cut(document)\n",
    "        wordCount = {}\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:               \n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = currentId;\n",
    "                    id2word[currentId] = word;\n",
    "                    currentId += 1;\n",
    "                if word in wordCount:\n",
    "                    wordCount[word] += 1\n",
    "                else:\n",
    "                    wordCount[word] = 1\n",
    "        wordCounts.append(wordCount);\n",
    "    \n",
    "    # length of dictionary\n",
    "    M = len(word2id)  \n",
    "\n",
    "    # generate the document-word matrix\n",
    "    X = zeros([N, M], int8)\n",
    "    for word in word2id.keys():\n",
    "        j = word2id[word]\n",
    "        for i in range(0, N):\n",
    "            if word in wordCounts[i]:\n",
    "                X[i, j] = wordCounts[i][word];    \n",
    "\n",
    "    return N, M, word2id, id2word, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the default params and read the params from cmd\n",
    "datasetFilePath = '../data/PLSA/dataset3.txt'\n",
    "stopwordsFilePath = '../data/PLSA/stopwords.dic'\n",
    "K = 10    # number of topic\n",
    "maxIteration = 30\n",
    "threshold = 10.0\n",
    "topicWordsNum = 10\n",
    "docTopicDist = '../data/PLSA/docTopicDistribution.txt'\n",
    "topicWordDist = '../data/PLSA/topicWordDistribution.txt'\n",
    "dictionary = '../data/PLSA/dictionary.dic'\n",
    "topicWords = '../data/PLSA/topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5757)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lamda[i, j] : p(zj|di)\n",
    "lamda = random([N, K])\n",
    "\n",
    "# theta[i, j] : p(wj|zi)\n",
    "theta = random([K, M])\n",
    "\n",
    "# p[i, j, k] : p(zk|di,wj)\n",
    "p = zeros([N, M, K])\n",
    "\n",
    "initializeParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬ä¸‰æ­¥æ˜¯æˆ‘ä»¬éœ€è¦è®¡ç®—Eï¼ŒM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EStep():\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            denominator = 0;\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
    "                denominator += p[i, j, k];\n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] = 0;\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] /= denominator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MStep():\n",
    "    # update theta\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] = 0\n",
    "            for i in range(0, N):\n",
    "                theta[k, j] += X[i, j] * p[i, j, k]\n",
    "            denominator += theta[k, j]\n",
    "        if denominator == 0:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] = 1.0 / M\n",
    "        else:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] /= denominator\n",
    "        \n",
    "    # update lamda\n",
    "    for i in range(0, N):\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] = 0\n",
    "            denominator = 0\n",
    "            for j in range(0, M):\n",
    "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
    "                denominator += X[i, j];\n",
    "            if denominator == 0:\n",
    "                lamda[i, k] = 1.0 / K\n",
    "            else:\n",
    "                lamda[i, k] /= denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ç€æˆ‘ä»¬å®šä¹‰æœ€å¤§ä¼¼ç„¶å‡½æ•°\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/36dda384ca41b649d5896500a3c6bdb63ecdcb64)\n",
    "ç¬¬ä¸€éƒ¨åˆ†æ˜¯å®šå€¼ï¼Œæˆ‘ä»¬ä¸»è¦ä¼˜åŒ–ç¬¬äºŒéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the log likelihood\n",
    "def LogLikelihood():\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            tmp = 0\n",
    "            for k in range(0, K):\n",
    "                tmp += theta[k, j] * lamda[i, k]\n",
    "            if tmp > 0:\n",
    "                loglikelihood += X[i, j] * log(tmp)\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# output the params of model and top words of topics to files\n",
    "def output():\n",
    "    # document-topic distribution\n",
    "    file = codecs.open(docTopicDist,'w','utf-8')\n",
    "    for i in range(0, N):\n",
    "        tmp = ''\n",
    "        for j in range(0, K):\n",
    "            tmp += str(lamda[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # topic-word distribution\n",
    "    file = codecs.open(topicWordDist,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        tmp = ''\n",
    "        for j in range(0, M):\n",
    "            tmp += str(theta[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # dictionary\n",
    "    file = codecs.open(dictionary,'w','utf-8')\n",
    "    for i in range(0, M):\n",
    "        file.write(id2word[i] + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # top words of each topic\n",
    "    file = codecs.open(topicWords,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        topicword = []\n",
    "        ids = theta[i, :].argsort()\n",
    "        for j in ids:\n",
    "            topicword.insert(0, id2word[j])\n",
    "        tmp = ''\n",
    "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
    "            tmp += word + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2017-10-11 15:34:36 ]  1  iteration   -119117.603574\n",
      "[ 2017-10-11 15:35:15 ]  2  iteration   -116686.457897\n",
      "[ 2017-10-11 15:35:55 ]  3  iteration   -113447.248935\n",
      "[ 2017-10-11 15:36:34 ]  4  iteration   -110041.623584\n",
      "[ 2017-10-11 15:37:14 ]  5  iteration   -107015.812203\n",
      "[ 2017-10-11 15:37:53 ]  6  iteration   -104487.345965\n",
      "[ 2017-10-11 15:38:33 ]  7  iteration   -102420.812678\n",
      "[ 2017-10-11 15:39:12 ]  8  iteration   -100820.586452\n",
      "[ 2017-10-11 15:39:52 ]  9  iteration   -99754.3630987\n",
      "[ 2017-10-11 15:40:31 ]  10  iteration   -99132.5837025\n",
      "[ 2017-10-11 15:41:10 ]  11  iteration   -98773.0945751\n",
      "[ 2017-10-11 15:41:50 ]  12  iteration   -98544.0665881\n",
      "[ 2017-10-11 15:42:33 ]  13  iteration   -98366.1570079\n",
      "[ 2017-10-11 15:43:12 ]  14  iteration   -98181.4161962\n",
      "[ 2017-10-11 15:43:52 ]  15  iteration   -97969.1043667\n",
      "[ 2017-10-11 15:44:31 ]  16  iteration   -97753.8946986\n",
      "[ 2017-10-11 15:45:10 ]  17  iteration   -97570.5593186\n",
      "[ 2017-10-11 15:45:50 ]  18  iteration   -97442.3102903\n",
      "[ 2017-10-11 15:46:29 ]  19  iteration   -97359.182354\n",
      "[ 2017-10-11 15:47:08 ]  20  iteration   -97299.9882036\n",
      "[ 2017-10-11 15:47:48 ]  21  iteration   -97257.0883117\n",
      "[ 2017-10-11 15:48:27 ]  22  iteration   -97220.6533569\n",
      "[ 2017-10-11 15:49:06 ]  23  iteration   -97189.8405976\n",
      "[ 2017-10-11 15:49:46 ]  24  iteration   -97168.4280998\n",
      "[ 2017-10-11 15:50:25 ]  25  iteration   -97150.2249148\n",
      "[ 2017-10-11 15:51:04 ]  26  iteration   -97135.9681279\n",
      "[ 2017-10-11 15:51:44 ]  27  iteration   -97118.4406008\n",
      "[ 2017-10-11 15:52:26 ]  28  iteration   -97104.7951924\n",
      "[ 2017-10-11 15:53:08 ]  29  iteration   -97099.3703603\n"
     ]
    }
   ],
   "source": [
    "# EM algorithm\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "for i in range(0, maxIteration):\n",
    "    EStep()\n",
    "    MStep()\n",
    "    newLoglikelihood = LogLikelihood()\n",
    "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood\n",
    "\n",
    "output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å­©å­ çº¢è¡£ æ½˜å…ˆç”Ÿ å­—æ¡ äº¤è­¦ å‘Šè¯‰ æœªæˆå¹´äºº æ€§ä¾µ ç½‘å‹ è§†é¢‘ \r\n",
      "æ–°æ–° ç”·å­ æ°‘è­¦ å­©å­ å­¦æ ¡ ç‹æŸ ç”·ç«¥ è­¦å®˜è¯ åˆ˜å…ˆç”Ÿ é…’å§ \r\n",
      "å‘¨æ°ä¼¦ è®°è€… åŒ—äº¬ ä¹¦ç”»é™¢ å¾®ä¿¡ è­¦æ–¹ å…¬ä¼— å«Œç–‘äºº æ°‘è­¦ ç‹è¶… \r\n",
      "ä½•å¤© å¸æœº é¢åŒ…è½¦ å‰‘é”‹ å°å· å…¬äº¤è½¦ è€äºº è¾›æŸ ä¹˜å®¢ è®°è€… \r\n",
      "è½¦è¾† é«˜é€Ÿ é£™è½¦ äº‹æ•… å¹¿å· å‘ç”Ÿ äº¤è­¦ åˆ†çº¢ è®°è€… è¡Œé©¶ \r\n",
      "åˆ†çº¢ æ‘æ°‘ é“¶è¡Œ æ°‘è­¦ è…¾å†² å©†å©† å¤§å¢© åˆ˜æŸ é¡ºå¾· åœŸè±ª \r\n",
      "å­©å­ å®¶é•¿ æ±‚å©š æå¥³å£« å„¿å­ æœ‹å‹ äº¤è­¦ æ•™è‚² å°è‹ è€å¸ˆ \r\n",
      "æ‰‹æœº æ™¯åŒº æ¸¸å®¢ é¾™æ±  æ•‘æ´ æ‘æ°‘ æœæ•‘ èƒ¡æ• æ•‘æ´é˜Ÿ ç™»å±± \r\n",
      "æŠ¥è­¦ æ°‘è­¦ é©¬èµ›å…‹ è­¦æ–¹ å¾®åš å«ŒçŠ¯ æ‹¨æ‰“ æ²»ç–— ç…§ç‰‡ è¡Œæ”¿æ‹˜ç•™ \r\n",
      "åˆ˜å°§ æ²³æº å‡å¸ è®°è€… è­¦æ–¹ ç‰©æµ å¥³å£« ä¸¾æŠ¥ å¸¦èµ° åŒ»é™¢ \r\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/PLSA/topics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ä¸Šé¢çš„æœåŠ¡ä½¿ç”¨èµ·æ¥ç‰¹åˆ«çš„æ…¢ã€‚ã€‚ä½¿ç”¨\n",
    "\n",
    "ä¸‹é¢æˆ‘ä½¿ç”¨gensimä¸­çš„plsaæ–¹æ³•ï¼Œåœ¨gensimä¸­å¹¶æ²¡æœ‰ä¸“é—¨å®ç°plsaï¼Œåªæœ‰LDAï¼Œé€šè¿‡è®¾ç½®LDAå‚æ•°å¯ä»¥æ¥å®ç°plsaï¼Œæ‰€æœ‰æˆ‘ä»¬å…ˆæ¥çœ‹LDAçš„ã€‚\n",
    "\n",
    "### 3. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ç‹¬ç«‹åŒåˆ†å¸ƒ independent and identically distributed (i.i.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log to external log file\n",
    "# import logging\n",
    "# logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set up log to terminal\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-12 16:23:20,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-10-12 16:23:22,581 : INFO : built Dictionary(27466 unique tokens: ['bahia', 'cocoa', 'review', 'shower', 'continued']...) from 9601 documents (total 1214137 corpus positions)\n",
      "2017-10-12 16:23:22,654 : INFO : discarding 24027 tokens: [('bahia', 1), ('shower', 6), ('the', 7371), ('in', 6381), ('alleviating', 2), ('and', 6869), ('for', 5439), ('temporao', 2), ('humidity', 3), ('restored', 18)]...\n",
      "2017-10-12 16:23:22,655 : INFO : keeping 3439 tokens which were in no less than 20 and no more than 4800 (=50.0%) documents\n",
      "2017-10-12 16:23:22,668 : INFO : resulting dictionary: Dictionary(3439 unique tokens: ['cocoa', 'review', 'continued', 'throughout', 'week']...)\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, dictionary =  getTfidfCorups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-12 16:23:40,912 : INFO : using symmetric alpha at 0.01\n",
      "2017-10-12 16:23:40,913 : INFO : using symmetric eta at 0.0002907822041291073\n",
      "2017-10-12 16:23:40,915 : INFO : using serial LDA version on this node\n",
      "2017-10-12 16:23:46,279 : INFO : running online (single-pass) LDA training, 100 topics, 1 passes over the supplied corpus of 9601 documents, updating model once every 2000 documents, evaluating perplexity every 9601 documents, iterating 500x with a convergence threshold of 0.001000\n",
      "2017-10-12 16:23:46,281 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2017-10-12 16:23:46,487 : INFO : PROGRESS: pass 0, at document #2000/9601\n",
      "2017-10-12 16:23:52,232 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:23:52,882 : INFO : topic #99 (0.010): 0.013*\"v\" + 0.009*\"mln\" + 0.007*\"ct\" + 0.007*\"ford\" + 0.007*\"net\" + 0.006*\"sale\" + 0.006*\"ltd\" + 0.005*\"pct\" + 0.005*\"shr\" + 0.005*\"split\"\n",
      "2017-10-12 16:23:52,884 : INFO : topic #38 (0.010): 0.018*\"v\" + 0.010*\"march\" + 0.010*\"ct\" + 0.008*\"sale\" + 0.007*\"billion\" + 0.007*\"store\" + 0.007*\"president\" + 0.006*\"dlrs\" + 0.006*\"shr\" + 0.006*\"foot\"\n",
      "2017-10-12 16:23:52,885 : INFO : topic #88 (0.010): 0.010*\"v\" + 0.009*\"ct\" + 0.006*\"tax\" + 0.006*\"group\" + 0.006*\"mart\" + 0.006*\"offer\" + 0.006*\"grant\" + 0.005*\"coffee\" + 0.005*\"pct\" + 0.005*\"center\"\n",
      "2017-10-12 16:23:52,887 : INFO : topic #91 (0.010): 0.016*\"reject\" + 0.013*\"hotel\" + 0.012*\"gatt\" + 0.012*\"store\" + 0.011*\"pct\" + 0.010*\"franklin\" + 0.009*\"offering\" + 0.009*\"insured\" + 0.009*\"productivity\" + 0.009*\"ohio\"\n",
      "2017-10-12 16:23:52,888 : INFO : topic #24 (0.010): 0.028*\"v\" + 0.014*\"net\" + 0.013*\"ct\" + 0.012*\"mln\" + 0.011*\"rev\" + 0.010*\"shr\" + 0.009*\"dlrs\" + 0.007*\"bbl\" + 0.006*\"tonne\" + 0.005*\"4th\"\n",
      "2017-10-12 16:23:52,891 : INFO : topic diff=78.508443, rho=1.000000\n",
      "2017-10-12 16:23:53,099 : INFO : PROGRESS: pass 0, at document #4000/9601\n",
      "2017-10-12 16:23:59,346 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:23:59,833 : INFO : topic #83 (0.010): 0.025*\"coin\" + 0.020*\"billion\" + 0.019*\"fund\" + 0.019*\"loan\" + 0.018*\"versus\" + 0.018*\"loss\" + 0.018*\"asset\" + 0.018*\"dlrs\" + 0.015*\"funding\" + 0.014*\"latest\"\n",
      "2017-10-12 16:23:59,835 : INFO : topic #89 (0.010): 0.014*\"hughes\" + 0.014*\"billion\" + 0.012*\"split\" + 0.010*\"declares\" + 0.010*\"baker\" + 0.009*\"authorized\" + 0.009*\"treasury\" + 0.009*\"appropriate\" + 0.008*\"pct\" + 0.008*\"dlrs\"\n",
      "2017-10-12 16:23:59,836 : INFO : topic #80 (0.010): 0.016*\"ecuador\" + 0.009*\"export\" + 0.009*\"rate\" + 0.008*\"v\" + 0.008*\"profit\" + 0.008*\"treasury\" + 0.007*\"crown\" + 0.007*\"bill\" + 0.007*\"tonne\" + 0.007*\"dollar\"\n",
      "2017-10-12 16:23:59,837 : INFO : topic #60 (0.010): 0.019*\"v\" + 0.011*\"guilder\" + 0.011*\"share\" + 0.009*\"net\" + 0.009*\"ct\" + 0.009*\"completed\" + 0.008*\"sale\" + 0.008*\"ag\" + 0.008*\"common\" + 0.008*\"mln\"\n",
      "2017-10-12 16:23:59,838 : INFO : topic #30 (0.010): 0.016*\"canada\" + 0.015*\"bond\" + 0.012*\"king\" + 0.011*\"canadian\" + 0.009*\"cyclops\" + 0.008*\"conversion\" + 0.008*\"dlrs\" + 0.007*\"issue\" + 0.007*\"v\" + 0.007*\"group\"\n",
      "2017-10-12 16:23:59,840 : INFO : topic diff=0.923743, rho=0.707107\n",
      "2017-10-12 16:24:00,053 : INFO : PROGRESS: pass 0, at document #6000/9601\n",
      "2017-10-12 16:24:06,766 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:07,194 : INFO : topic #45 (0.010): 0.011*\"tonne\" + 0.009*\"price\" + 0.009*\"opec\" + 0.008*\"export\" + 0.007*\"oil\" + 0.007*\"output\" + 0.006*\"trade\" + 0.006*\"he\" + 0.006*\"official\" + 0.006*\"import\"\n",
      "2017-10-12 16:24:07,195 : INFO : topic #59 (0.010): 0.025*\"preferred\" + 0.017*\"relief\" + 0.014*\"saudi\" + 0.011*\"wind\" + 0.011*\"share\" + 0.011*\"life\" + 0.011*\"floating\" + 0.010*\"halted\" + 0.010*\"australia\" + 0.010*\"release\"\n",
      "2017-10-12 16:24:07,197 : INFO : topic #9 (0.010): 0.021*\"agreement\" + 0.021*\"buy\" + 0.019*\"asset\" + 0.018*\"subject\" + 0.017*\"inc\" + 0.016*\"undisclosed\" + 0.015*\"unit\" + 0.015*\"acquisition\" + 0.014*\"approval\" + 0.014*\"venture\"\n",
      "2017-10-12 16:24:07,198 : INFO : topic #58 (0.010): 0.016*\"bankamerica\" + 0.010*\"bank\" + 0.009*\"tokyo\" + 0.007*\"rate\" + 0.007*\"bond\" + 0.006*\"provides\" + 0.006*\"competitor\" + 0.006*\"creditor\" + 0.006*\"pct\" + 0.006*\"issue\"\n",
      "2017-10-12 16:24:07,199 : INFO : topic #5 (0.010): 0.031*\"gnp\" + 0.023*\"3rd\" + 0.015*\"alberta\" + 0.014*\"pct\" + 0.013*\"unchanged\" + 0.013*\"rise\" + 0.011*\"realty\" + 0.011*\"central\" + 0.009*\"nine\" + 0.009*\"know\"\n",
      "2017-10-12 16:24:07,201 : INFO : topic diff=1.164674, rho=0.577350\n",
      "2017-10-12 16:24:07,407 : INFO : PROGRESS: pass 0, at document #8000/9601\n",
      "2017-10-12 16:24:13,439 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:13,765 : INFO : topic #19 (0.010): 0.084*\"saving\" + 0.029*\"association\" + 0.028*\"loan\" + 0.024*\"home\" + 0.023*\"certificate\" + 0.021*\"institutional\" + 0.018*\"federal\" + 0.017*\"deposit\" + 0.014*\"kansa\" + 0.014*\"bank\"\n",
      "2017-10-12 16:24:13,766 : INFO : topic #3 (0.010): 0.079*\"gencorp\" + 0.036*\"telephone\" + 0.034*\"telecommunication\" + 0.033*\"communication\" + 0.025*\"line\" + 0.022*\"los\" + 0.022*\"angeles\" + 0.019*\"borrowing\" + 0.019*\"credit\" + 0.018*\"john\"\n",
      "2017-10-12 16:24:13,767 : INFO : topic #60 (0.010): 0.023*\"placement\" + 0.017*\"participation\" + 0.013*\"share\" + 0.013*\"ag\" + 0.013*\"car\" + 0.011*\"volkswagen\" + 0.011*\"consecutive\" + 0.010*\"guilder\" + 0.010*\"florida\" + 0.009*\"v\"\n",
      "2017-10-12 16:24:13,768 : INFO : topic #69 (0.010): 0.030*\"medical\" + 0.018*\"freight\" + 0.017*\"offering\" + 0.014*\"license\" + 0.013*\"att\" + 0.012*\"file\" + 0.012*\"share\" + 0.010*\"shelf\" + 0.009*\"underwriter\" + 0.009*\"exclusive\"\n",
      "2017-10-12 16:24:13,769 : INFO : topic #73 (0.010): 0.035*\"seaman\" + 0.030*\"painewebber\" + 0.028*\"bankruptcy\" + 0.024*\"furniture\" + 0.024*\"reorganization\" + 0.020*\"copper\" + 0.017*\"basic\" + 0.016*\"outside\" + 0.014*\"generation\" + 0.013*\"court\"\n",
      "2017-10-12 16:24:13,771 : INFO : topic diff=1.326013, rho=0.500000\n",
      "2017-10-12 16:24:26,316 : INFO : -13.694 per-word bound, 13256.3 perplexity estimate based on a held-out corpus of 1601 documents with 9081 words\n",
      "2017-10-12 16:24:26,318 : INFO : PROGRESS: pass 0, at document #9601/9601\n",
      "2017-10-12 16:24:31,009 : INFO : merging changes from 1601 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:31,288 : INFO : topic #50 (0.010): 0.047*\"budget\" + 0.031*\"rating\" + 0.023*\"standard\" + 0.021*\"deficit\" + 0.021*\"aa\" + 0.017*\"lowered\" + 0.016*\"tax\" + 0.015*\"debt\" + 0.015*\"callable\" + 0.014*\"upgraded\"\n",
      "2017-10-12 16:24:31,290 : INFO : topic #30 (0.010): 0.078*\"cyclops\" + 0.047*\"dixons\" + 0.026*\"canada\" + 0.026*\"conversion\" + 0.019*\"canadian\" + 0.019*\"contingent\" + 0.018*\"zero\" + 0.017*\"king\" + 0.017*\"supply\" + 0.016*\"offer\"\n",
      "2017-10-12 16:24:31,291 : INFO : topic #60 (0.010): 0.026*\"placement\" + 0.018*\"ag\" + 0.016*\"participation\" + 0.015*\"fda\" + 0.014*\"facility\" + 0.013*\"volkswagen\" + 0.013*\"car\" + 0.012*\"florida\" + 0.011*\"share\" + 0.011*\"model\"\n",
      "2017-10-12 16:24:31,292 : INFO : topic #54 (0.010): 0.030*\"registration\" + 0.029*\"filed\" + 0.028*\"offering\" + 0.023*\"file\" + 0.022*\"security\" + 0.017*\"covering\" + 0.016*\"california\" + 0.016*\"statement\" + 0.015*\"incurred\" + 0.014*\"county\"\n",
      "2017-10-12 16:24:31,293 : INFO : topic #86 (0.010): 0.046*\"bill\" + 0.035*\"acre\" + 0.031*\"danish\" + 0.029*\"usda\" + 0.028*\"qualified\" + 0.024*\"crop\" + 0.021*\"chase\" + 0.018*\"farmer\" + 0.018*\"program\" + 0.017*\"manhattan\"\n",
      "2017-10-12 16:24:31,295 : INFO : topic diff=1.387438, rho=0.447214\n"
     ]
    }
   ],
   "source": [
    "# extract 4 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations\n",
    "lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4241091686516862\t Topic: 0.019*\"philippine\" + 0.017*\"trade\" + 0.016*\"cocoa\" + 0.013*\"bill\" + 0.011*\"taiwan\" + 0.011*\"tariff\" + 0.010*\"indonesia\" + 0.010*\"bank\" + 0.008*\"baldrige\" + 0.008*\"senate\"\n",
      "Score: 0.24113685576355734\t Topic: 0.009*\"tonne\" + 0.009*\"export\" + 0.008*\"he\" + 0.007*\"trade\" + 0.007*\"economic\" + 0.007*\"price\" + 0.006*\"import\" + 0.006*\"oil\" + 0.006*\"minister\" + 0.006*\"country\"\n",
      "Score: 0.11356546789795555\t Topic: 0.060*\"tonne\" + 0.022*\"exporter\" + 0.022*\"prudential\" + 0.021*\"wheat\" + 0.020*\"shipment\" + 0.019*\"park\" + 0.019*\"palm\" + 0.017*\"corn\" + 0.016*\"vegetable\" + 0.016*\"grain\"\n",
      "Score: 0.04811582951565254\t Topic: 0.017*\"moody\" + 0.017*\"shell\" + 0.016*\"oil\" + 0.015*\"crude\" + 0.015*\"brown\" + 0.015*\"coca\" + 0.015*\"cited\" + 0.014*\"cola\" + 0.012*\"downgrade\" + 0.011*\"bbl\"\n",
      "Score: 0.04196684828242752\t Topic: 0.020*\"yen\" + 0.017*\"latin\" + 0.017*\"dollar\" + 0.017*\"ecuador\" + 0.017*\"rio\" + 0.014*\"rate\" + 0.013*\"economy\" + 0.013*\"cooperate\" + 0.012*\"speaking\" + 0.012*\"nation\"\n",
      "Score: 0.035413358612613896\t Topic: 0.023*\"agreement\" + 0.022*\"inc\" + 0.020*\"unit\" + 0.019*\"acquisition\" + 0.019*\"undisclosed\" + 0.018*\"buy\" + 0.016*\"merger\" + 0.016*\"subsidiary\" + 0.016*\"approval\" + 0.014*\"sell\"\n"
     ]
    }
   ],
   "source": [
    "# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.\n",
    "for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n"
     ]
    }
   ],
   "source": [
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [lda[c] for c in corpus_tfidf]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors_matrix = gensim.matutils.corpus2dense(vectors, num_terms=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 100)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.transpose(vectors_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors_matrix = np.transpose(vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4367 / 4858) correct - 89.89%\n",
      "  done in 0.492sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(vectors_matrix[:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(vectors_matrix[4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥è¯´ä½¿ç”¨ldaåã€‚åè€Œæ•ˆæœä¸å¥½äº†å‘¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áü≠ËØ≠ÊåñÊéòÊñπÊ≥ï‰ªãÁªç\n",
    "\n",
    "Á¨¨‰∏ÄÂ§ßÁ±ªÔºöUnigram topic modeling ‰∏ÄÂÖÉÁöÑ‰∏ªÈ¢òÊ®°ÂûãÔºå‰∏ªË¶ÅÂåÖÊã¨Ôºö\n",
    "\n",
    "1. Latent Semantic Analysis (LSA)\n",
    "2. Probablistic Latent Semantic Analysis ÔºàpLSAÔºâ\n",
    "3. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "‰∏ãÈù¢ÂºÄÂßãÂØπËøôÂá†ÁßçÊ®°ÂûãÁöÑ‰ªãÁªç\n",
    "\n",
    "### 1.  LSA\n",
    "ÂèÇËÄÉÂçöÊñá[ÂÖ≥‰∫é LSAÔºàLatent Semantic AnalysisÔºâ‰∏ªÈ¢òÊ®°ÂûãÁöÑ‰∏™‰∫∫ÁêÜËß£](http://blog.csdn.net/cang_sheng_ta_ge/article/details/46708515)ÔºåÂè¶Â§ñ‰∏ÄÁØáËã±ÊñáÊñáÁ´†[Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "\n",
    "ÂÖ∂‰∏ªË¶ÅÊÄùÊÉ≥ÊÑüËßâÊúâÁÇπÁ±ª‰ºº‰∫éËØçÂµåÂÖ•ÔºåÊàë‰ª¨ÂÅáËÆæÊúâÂ•ΩÂ§öËØçÔºåÈÇ£ÂèØ‰ª•Â∞ÜÊñáÊ°£Áî®ËØçÊù•ËøõË°åone-hot encodingÔºå‰ΩÜÊòØËøôÊ†∑‰ºöÈÄ†ÊàêÊñáÁ´†Â§ßÈáèÁöÑÁ®ÄÁñèË°®Á§∫Ôºå‰∫éÊòØÊàë‰ª¨ÊÉ≥Ê≥ïÂ∞±ÊòØÊÄé‰πàËÉΩÂ§üÂ∞ÜÁ®ÄÁñèÁü©ÈòµÂèò‰∏∫Á®†ÂØÜÁü©ÈòµÔºå‰∏Ä‰∏™ÂÅöÊ≥ïÂ∞±ÊòØÁü©ÈòµÂàÜËß£„ÄÇ\n",
    "\n",
    "‰∏ãÈù¢‰ª£Á†ÅÂèÇËÄÉ https://github.com/chrisjmccormick/LSA_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ê≠§Â§ÑÁöÑÊï∞ÊçÆ‰ΩøÁî®ÁöÑÊòØË∑ØÈÄèÁ§æÁöÑÊï∞ÊçÆÔºåÂ§ßÊ¶ÇÁ±ªÂà´Êúâ100Â∑¶Âè≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n"
     ]
    }
   ],
   "source": [
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËÆ°ÁÆó LSA Á¨¨‰∏ÄÊ≠•Â∞±ÊòØÂ∞±Êåâtf-idfÂÄºÔºåÊàë‰ª¨Áõ¥Êé•‰ΩøÁî® sklearn ‰∏≠ÁöÑÊ®°Âùó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Êàë‰ª¨ÂéªÈô§‰∫ÜÂÅúÁî®ËØç\n",
    "# ËøáÊª§ÊéâÂá∫Áé∞ÊñáÊ°£Ë∂ÖËøá50%ÁöÑËØç\n",
    "# ËøáÊª§ÊéâÊúÄÂ∞èÂá∫Áé∞2Ê¨°ÁöÑËØç\n",
    "# ÊåëÈÄâÂá∫Ââç1w‰∏™ËØç\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: 10000\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4743, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰∏ã‰∏ÄÊ≠•Êàë‰ª¨Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÂØπÈ´òÁª¥ÁöÑÁâπÂæÅÁü©ÈòµËøõË°åÂàÜËß£ÔºåSVDÂ•áÂºÇÂÄºÂàÜËß£„ÄÇ\n",
    "\n",
    "X_train_tfidf ‰∏≠ÊØè‰∏ÄË°å‰ª£Ë°®‰∏ÄÁØáÊñáÁ´†ÔºåÊØè‰∏ÄÂàó‰ª£Ë°®‰∏Ä‰∏™ËØçÔºåÊØè‰∏™Êï∞ÂÄºÈÉΩÊòØtf-idfÂÄº„ÄÇ\n",
    "\n",
    "Â•áÂºÇÂÄºÂàÜËß£ÂÅöÁöÑ‰∫ãÊÉÖÂ∞±ÊòØÂ∞ÜÈ´òÁ∫¨Â∫¶Á®ÄÁñèÁü©ÈòµÂàÜËß£‰∏∫3‰∏™Áü©ÈòµËøû‰πò„ÄÇÁΩë‰∏äÊâæÂà∞‰∏ÄÁØáÈùûÂ∏∏Â•ΩÁöÑÊñáÁ´†Ôºö[Â•áÂºÇÂÄºÂàÜËß£ (SVD) --- Âá†‰ΩïÊÑè‰πâ](http://blog.sciencenet.cn/blog-696950-699432.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êàë‰ª¨ÂÖàÂØπËÆ≠ÁªÉÊï∞ÊçÆÊ±ÇÂèñsvdÔºåÁÑ∂ÂêéÂØπÈΩêËøõË°åÊ≠£ÂàôÂåñÔºåÂæóÂà∞Âçï‰ΩçÂêëÈáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂæóÂà∞ËΩ¨Êç¢ÂêéÁöÑÁâπÂæÅÂÄºÔºåÂπ∂‰∏îËøô‰∫õÁâπÂæÅÂç†ÊÄªÁöÑ‰ø°ÊÅØÈáè‰∏∫27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Explained variance of the SVD step: 27%\n"
     ]
    }
   ],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰∏ãÈù¢ÂºÄÂßãÂØπÊñáÁ´†ËøõË°åÂàÜÁ±ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4471 / 4858) correct - 92.03%\n",
      "  done in 1.404sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4561 / 4858) correct - 93.89%\n",
      "    done in 0.749sec\n"
     ]
    }
   ],
   "source": [
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(X_test_lsa)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim‰ΩøÁî®\n",
    "‰ΩøÁî® gensim Êù•ÂÅö LSA ÁöÑÂàÜÊûêÔºåLatent Semantic Indexing (also known as Latent Semantic Analysis)ÔºåÂú® gensim ‰∏≠ÂØπÂ∫îÁöÑÊ®°ÂùóÊòØmodels.lsimodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Êï¥‰∏™Â§ÑÁêÜËøáÁ®ãÂèØ‰ª•ÂàÜ‰∏∫4Ê≠•Ôºö\n",
    "\n",
    "1. Â∞ÜÊñáÊ°£ÂàÜËØç\n",
    "2. ÂçïËØçLemmatize\n",
    "3. idfÂÄº\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docTokenizer(docs):\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    newdocs = []\n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx].lower()  # Convert to lowercase.\n",
    "        newdocs.append(tokenizer.tokenize(doc))  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    newdocs = [[token for token in doc if not token.isnumeric()] for doc in newdocs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    newdocs = [[token for token in doc if len(token) > 1] for doc in newdocs]\n",
    "    \n",
    "    return newdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_tokens = docTokenizer(X_train_raw)\n",
    "x_test_tokens = docTokenizer(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "x_train_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_train_tokens]\n",
    "x_test_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰∏ã‰∏ÄÊ≠•Êàë‰ª¨Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÂª∫Á´ãcorups‰∫ÜÔºåÂéªÈô§Ë∂ÖËøáÂú®‰∏ÄÂçäÊñáÊ°£ÈÉΩÂá∫Áé∞ÁöÑËØçÔºåÂéªÈô§Âá∫Áé∞ÊñáÊ°£Êï∞Â∞è‰∫é20ÁöÑËØç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:24:20,453 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-09-30 16:24:22,217 : INFO : built Dictionary(27466 unique tokens: ['bahia', 'cocoa', 'review', 'shower', 'continued']...) from 9601 documents (total 1214137 corpus positions)\n",
      "2017-09-30 16:24:22,279 : INFO : discarding 24027 tokens: [('bahia', 1), ('shower', 6), ('the', 7371), ('in', 6381), ('alleviating', 2), ('and', 6869), ('for', 5439), ('temporao', 2), ('humidity', 3), ('restored', 18)]...\n",
      "2017-09-30 16:24:22,281 : INFO : keeping 3439 tokens which were in no less than 20 and no more than 4800 (=50.0%) documents\n",
      "2017-09-30 16:24:22,293 : INFO : resulting dictionary: Dictionary(3439 unique tokens: ['cocoa', 'review', 'continued', 'throughout', 'week']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(np.concatenate((x_train_tokens,x_test_tokens)))\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ê≠§Â§ÑÂéªÈô§ÂÆåÂêéÔºåÂ≠óÂÖ∏Â∞±Ââ© 3439 ‰∫ÜÔºå‰∏ãÈù¢Êàë‰ª¨Â§ÑÁêÜËØ≠Êñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "# Vectorize data.\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "# Bag-of-words representation of the documents.\n",
    "corups = [dictionary.doc2bow(doc) for doc in np.concatenate((x_train_tokens,x_test_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel,TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:42:37,165 : INFO : using serial LSI version on this node\n",
      "2017-09-30 16:42:37,167 : INFO : updating model with new documents\n",
      "2017-09-30 16:42:38,488 : INFO : preparing a new chunk of documents\n",
      "2017-09-30 16:42:38,674 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-09-30 16:42:38,676 : INFO : 1st phase: constructing (3439, 200) action matrix\n",
      "2017-09-30 16:42:38,922 : INFO : orthonormalizing (3439, 200) action matrix\n",
      "2017-09-30 16:42:39,723 : INFO : 2nd phase: running dense svd on (200, 9601) matrix\n",
      "2017-09-30 16:42:39,900 : INFO : computing the final decomposition\n",
      "2017-09-30 16:42:39,903 : INFO : keeping 100 factors (discarding 20.809% of energy spectrum)\n",
      "2017-09-30 16:42:39,909 : INFO : processed documents up to #9601\n",
      "2017-09-30 16:42:39,912 : INFO : topic #0(24.459): -0.640*\"v\" + -0.364*\"ct\" + -0.289*\"net\" + -0.261*\"loss\" + -0.228*\"shr\" + -0.196*\"mln\" + -0.173*\"rev\" + -0.155*\"profit\" + -0.142*\"qtr\" + -0.098*\"oper\"\n",
      "2017-09-30 16:42:39,914 : INFO : topic #1(16.559): 0.180*\"pct\" + -0.173*\"v\" + 0.163*\"bank\" + 0.140*\"share\" + 0.136*\"billion\" + 0.122*\"dlrs\" + 0.113*\"will\" + 0.110*\"he\" + 0.110*\"is\" + 0.108*\"be\"\n",
      "2017-09-30 16:42:39,916 : INFO : topic #2(13.214): -0.386*\"qtly\" + -0.366*\"div\" + -0.330*\"ct\" + -0.301*\"record\" + -0.268*\"prior\" + -0.262*\"pay\" + -0.255*\"april\" + 0.217*\"loss\" + -0.185*\"dividend\" + -0.180*\"quarterly\"\n",
      "2017-09-30 16:42:39,919 : INFO : topic #3(9.648): 0.368*\"share\" + -0.217*\"billion\" + 0.212*\"offering\" + 0.195*\"stock\" + 0.182*\"common\" + 0.151*\"inc\" + -0.150*\"bank\" + 0.129*\"debenture\" + 0.123*\"convertible\" + 0.122*\"company\"\n",
      "2017-09-30 16:42:39,921 : INFO : topic #4(9.361): -0.794*\"loss\" + -0.294*\"profit\" + 0.244*\"v\" + 0.176*\"mln\" + 0.165*\"net\" + 0.091*\"billion\" + -0.086*\"oper\" + -0.083*\"qtly\" + 0.082*\"pct\" + 0.077*\"shr\"\n"
     ]
    }
   ],
   "source": [
    "lsi = LsiModel(corpus_tfidf, num_topics=100,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [[item[1] for item in vec] for vec in  lsi[corpus_tfidf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858 9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw),len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4593 / 4858) correct - 94.55%\n",
      "  done in 0.699sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(vectors[:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(vectors[4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÈÄöËøá‰∏äÈù¢ÁöÑÈ¢ÑÂ§ÑÁêÜÔºåÊàë‰ª¨Ê®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûúÂèàÊúâ‰∫ÜÊèêÂçá üòÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA ÁöÑÂª∂‰º∏\n",
    "ÁúãÂà∞‰∏äÈù¢ LSA ÊñπÊ≥ïÁöÑÊó∂ÂÄôÔºåÂÖ∂ÂÆûÊàëÂ∞±ÊÉ≥Âà∞‰∫Ü word embedingÔºåÈÇ£Êàë‰ª¨ËÉΩÂ§üÁî®Á•ûÁªèÁΩëÁªúÊù•ÂÅöLSAÂòõÔºåÊÄùË∑ØÔºöÊàë‰ª¨Â∞ÜdocumentËøõË°åembedingÔºåÁÑ∂ÂêéÂ∞ÜËØç‰πüËøõË°åembeddingÔºåÁÑ∂ÂêéÁªèËøá‰∏Ä‰∏™3Â±ÇÂÖ®ËøûÊé•ÔºåËæìÂá∫ËØçÂú®document‰∏≠Âá∫Áé∞ÁöÑÊ¨°Êï∞ÔºåÊàë‰ª¨‰∏çÊñ≠ÁöÑÊãüÂêà‰∏äÈù¢ÁöÑÁΩëÁªúÔºåÂ∞±ËÉΩÂæóÂà∞ document Âíå word ÁöÑÂêëÈáè‰∫Ü„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw)+len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInput = keras.Input((1,),name='word_input',dtype='int32')\n",
    "docInput = keras.Input((1,),name='doc_input',dtype='int32')\n",
    "# countInput = keras.Input((1,),name='count_input',dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_topic_num = 100\n",
    "wordEmbed = keras.layers.embeddings.Embedding(10000,latent_topic_num,input_length=1,\n",
    "                                              embeddings_initializer=keras.initializers.VarianceScaling(2),name='word_embed')\n",
    "docEmbed = keras.layers.embeddings.Embedding(9601,latent_topic_num,input_length=1,\n",
    "                                  embeddings_initializer=keras.initializers.VarianceScaling(2),name='doc_embed')\n",
    "word_embed = keras.layers.Reshape((latent_topic_num,))(wordEmbed(wordInput)) # can also use Flatten\n",
    "doc_embed = keras.layers.Reshape((latent_topic_num,))(docEmbed(docInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_16/Reshape:0' shape=(?, 100) dtype=float32>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vector = keras.layers.concatenate([word_embed,doc_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_9/concat:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_len = 3\n",
    "# layer_num = 10\n",
    "layer_nums = [20,10,5]\n",
    "for i in range(len(layer_nums)):\n",
    "    input_vector = keras.layers.Dense(layer_nums[i],\n",
    "                                      kernel_initializer=keras.initializers.VarianceScaling(2),\n",
    "                                     activation='relu')(input_vector)\n",
    "    \n",
    "y_predict = keras.layers.Dense(1,kernel_initializer=keras.initializers.VarianceScaling(2))(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model([wordInput,docInput],y_predict)\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰∏ãÈù¢Ê†πÊçÆ‰∏äÈù¢ÁöÑÊ®°ÂûãÔºåÊàë‰ª¨Êù•ÂÆöÂà∂Êàë‰ª¨ÁöÑËæìÂÖ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_vectorizer = CountVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_documents = np.concatenate((X_train_raw,X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_tf = dnn_vectorizer.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 10000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_frequence = doc_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInputs = []\n",
    "docInputs = []\n",
    "targetCounts = []\n",
    "for i in range(len(term_frequence)):\n",
    "    doc_frequence = term_frequence[i]\n",
    "    for j in range(len(doc_frequence)):\n",
    "        if doc_frequence[j] != 0:\n",
    "            docInputs.append(i)\n",
    "            wordInputs.append(j)\n",
    "            targetCounts.append(doc_frequence[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "doc_input (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word_embed (Embedding)           (None, 1, 5)          48005       word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "doc_embed (Embedding)            (None, 1, 5)          50000       doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)             (None, 5)             0           word_embed[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)             (None, 5)             0           doc_embed[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 10)            0           reshape_12[0][0]                 \n",
      "                                                                   reshape_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 20)            220         concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 10)            210         dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 5)             55          dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 1)             6           dense_26[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 98,496\n",
      "Trainable params: 98,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498923"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3101     \n",
      "Epoch 2/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3077     \n",
      "Epoch 3/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3062     \n",
      "Epoch 4/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3030     \n",
      "Epoch 5/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3010     \n",
      "Epoch 6/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2990     \n",
      "Epoch 7/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2963     \n",
      "Epoch 8/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2961     \n",
      "Epoch 9/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2949     \n",
      "Epoch 10/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2915     \n",
      "Epoch 11/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2895     \n",
      "Epoch 12/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2881     \n",
      "Epoch 13/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2870     \n",
      "Epoch 14/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2853     \n",
      "Epoch 15/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2831     \n",
      "Epoch 16/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2817     \n",
      "Epoch 17/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2801     \n",
      "Epoch 18/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2795     \n",
      "Epoch 19/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2769     \n",
      "Epoch 20/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2756     \n",
      "Epoch 21/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2743     \n",
      "Epoch 22/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2731     \n",
      "Epoch 23/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2715     \n",
      "Epoch 24/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2701     \n",
      "Epoch 25/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2692     \n",
      "Epoch 26/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2668     \n",
      "Epoch 27/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2654     \n",
      "Epoch 28/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2647     \n",
      "Epoch 29/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2639     \n",
      "Epoch 30/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2622     \n",
      "Epoch 31/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2613     \n",
      "Epoch 32/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2605     \n",
      "Epoch 33/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2592     \n",
      "Epoch 34/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2590     \n",
      "Epoch 35/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2578     \n",
      "Epoch 36/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2568     \n",
      "Epoch 37/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2574     \n",
      "Epoch 38/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2551     \n",
      "Epoch 39/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2530     \n",
      "Epoch 40/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2516     \n",
      "Epoch 41/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2502     \n",
      "Epoch 42/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2507     \n",
      "Epoch 43/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2484     \n",
      "Epoch 44/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2472     \n",
      "Epoch 45/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2469     \n",
      "Epoch 46/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2463     \n",
      "Epoch 47/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2448     \n",
      "Epoch 48/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2433     \n",
      "Epoch 49/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2420     \n",
      "Epoch 50/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2412     \n",
      "Epoch 51/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2418     \n",
      "Epoch 52/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2407     \n",
      "Epoch 53/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2402     \n",
      "Epoch 54/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2400     \n",
      "Epoch 55/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2397     \n",
      "Epoch 56/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2384     \n",
      "Epoch 57/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2368     \n",
      "Epoch 58/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2353     \n",
      "Epoch 59/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2346     \n",
      "Epoch 60/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2342     \n",
      "Epoch 61/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2339     \n",
      "Epoch 62/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2317     \n",
      "Epoch 63/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2322     \n",
      "Epoch 64/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2314     \n",
      "Epoch 65/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2307     \n",
      "Epoch 66/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2303     \n",
      "Epoch 67/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2289     \n",
      "Epoch 68/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2286     \n",
      "Epoch 69/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2277     \n",
      "Epoch 70/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2263     \n",
      "Epoch 71/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2261     \n",
      "Epoch 72/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2256     \n",
      "Epoch 73/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2254     \n",
      "Epoch 74/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2239     \n",
      "Epoch 75/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2241     \n",
      "Epoch 76/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2231     \n",
      "Epoch 77/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2224     \n",
      "Epoch 78/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2212     \n",
      "Epoch 79/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2199     \n",
      "Epoch 80/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2204     \n",
      "Epoch 81/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2207     \n",
      "Epoch 82/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2190     \n",
      "Epoch 83/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2185     \n",
      "Epoch 84/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2184     \n",
      "Epoch 85/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2177     \n",
      "Epoch 86/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2169     \n",
      "Epoch 87/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2163     \n",
      "Epoch 88/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2167     \n",
      "Epoch 89/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2155     \n",
      "Epoch 90/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2152     \n",
      "Epoch 91/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2145     \n",
      "Epoch 92/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2148     \n",
      "Epoch 93/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2139     \n",
      "Epoch 94/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2133     \n",
      "Epoch 95/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2128     \n",
      "Epoch 96/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2130     \n",
      "Epoch 97/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2118     \n",
      "Epoch 98/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2117     \n",
      "Epoch 99/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2106     \n",
      "Epoch 100/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2095     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d1079ef28>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array(wordInputs),np.array(docInputs)],np.array(targetCounts),batch_size=10000,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 5)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËøõË°åÂàÜÁ±ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4188 / 4858) correct - 86.21%\n",
      "    done in 0.515sec\n"
     ]
    }
   ],
   "source": [
    "#  (4183 / 4858) correct - 86.11%,\n",
    "#  0.4882 - 85.78% \n",
    "# 100‰∏™hiddenÔºå100Ê¨°Ôºåloss:0.3117 , correct - 86.39%\n",
    "# 100‰∏™hiddenÔºå100Ê¨°Ôºåloss:0.2095 , correct - 86.21%\n",
    "# ÂèØ‰ª•ËØ¥‰∏äÈù¢ËøôÁßçÂÅöÊ≥ï\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(model.get_weights()[1][:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(model.get_weights()[1][4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÁªìËÆ∫ÔºöÈÄöËøá‰∏äÈù¢ÁöÑdnnÊñπÊ≥ïÔºåbaselineÁöÑmodelÊïàÊûúÂπ∂‰∏çÊòØÂæàÂ•Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pLSA\n",
    "Âú®LSA‰∏≠ÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜtermÂíådocumentÁöÑÂÖ±Áé∞Áü©ÈòµÔºå\n",
    "![ÂõæÁâá](http://bos.nj.bpc.baidu.com/v1/agroup/afed8760421ed8b2b3d003e9b14f7f56673cc4a3)\n",
    "Ëøô‰∏™ÁöÑÊ†∏ÂøÉÂÅáËÆæÊòØÔºö‰ΩøÁî®ËØçË¢ãÊ®°ÂûãÊù•Ë°®Á§∫ÊñáÁ´†ÔºàÂç≥‰Ωø‰∏¢Â§±‰∫ÜtermÁöÑÈ°∫Â∫èÔºâ‰πüËÉΩÂæàÂ•ΩÁöÑ‰ª£Ë°®ÊñáÁ´†‰ø°ÊÅØÔºåLSA ÈÄöËøásvgÊñπÊ≥ïÔºåÂ∞ÜdocumentÊò†Â∞ÑÂà∞‰∏Ä‰∏™ÈöêÂêëÈáèÁ©∫Èó¥‰∏äÂéª„ÄÇ\n",
    "\n",
    "Êàë‰ª¨Êõ¥Ëøõ‰∏ÄÊ≠•ÔºåËÄÉËôëÂºïÂÖ•‰∏Ä‰∏™ÈöêÂê´ÂèòÈáè$z_k$ÔºåÊù•Ë°®Á§∫ÊñáÊ°£‰∏ªÈ¢òÔºåÂΩ¢ÊàêÁÆÄÂçïÁöÑË¥ùÂè∂ÊñØÁΩëÁªúÔºå\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/f67618811933d48654161ae53fe5f2d836ace8d8)\n",
    "\n",
    "ÈùûÂ∏∏Â•ΩÁöÑÊñáÊ°£ÂèØ‰ª•ÂèÇËÄÉÔºö\n",
    "\n",
    "[Ê¶ÇÁéáËØ≠Ë®ÄÊ®°ÂûãÂèäÂÖ∂ÂèòÂΩ¢Á≥ªÂàó (1)-PLSA Âèä EM ÁÆóÊ≥ï](http://blog.csdn.net/yangliuy/article/details/8330640)\n",
    "\n",
    "[Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰πã PLSA](http://zhikaizhang.cn/2016/06/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8BPLSA/)\n",
    "\n",
    "[‰∏ªÈ¢òÊ®°Âûã‰πã pLSA](http://blog.jqian.net/post/plsa.html)\n",
    "\n",
    "ÈáåÈù¢ÈáçË¶ÅÁöÑÁÇπÊòØEMÁöÑÊÄùÊÉ≥ÔºåÂç≥ÂÖàÂÅáËÆæÂèÇÊï∞$\\theta$Â∑≤Áü•ÔºåÁÑ∂ÂêéÊ±ÇÈöêÂèòÈáèÁöÑÂêéÈ™åÊ¶ÇÁéáÔºåÁÑ∂ÂêéÊ≠§Êó∂Áü•ÈÅìÈöêÂèòÈáèÂêéÔºåÂ∞±ÂèØ‰ª•ÂíåÊï∞ÊçÆX‰∏ÄËµ∑ÁªÑÊàêÂÆåÊï¥ÁöÑÊï∞ÊçÆÈõÜÔºåÊ≠§Êó∂ÂÜçÊù•Ê±ÇÊûÅÂÄº„ÄÇ\n",
    "\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/4a83a5cf5beaced4dbe535759365c1bab4b2862b)\n",
    "\n",
    "‰∏ãÈù¢ÊòØÂÖ∑‰ΩìÁöÑÁÆóÊ≥ïÂÆûÁé∞Ôºå‰ª£Á†ÅÂèÇËÄÉËá™Ôºöhttps://github.com/laserwave/PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import sys\n",
    "import jieba\n",
    "import re\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "‰ª£Á†Å‰∏≠‰ΩøÁî® $lamda[i,k]$ Ë°®Á§∫ÂèÇÊï∞ $p(z_k|d_i)$, Áî® $theta[k,j]$ Ë°®Á§∫ÂèÇÊï∞ $p(w_j|z_k)$, Áî® $p[i,j,k]$ Ë°®Á§∫ÈöêËóèÂèòÈáèÁöÑÂêéÈ™åÊ¶ÇÁéá $p(z_k|d_i,w_j)$„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Á¨¨‰∏ÄÊ≠•ÔºåÊàë‰ª¨ÈúÄË¶ÅÂàùÂßãÂåñÂèÇÊï∞ $p(z_k|d_i)$, $p(w_j|z_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeParameters():\n",
    "    for i in range(0, N):\n",
    "        normalization = sum(lamda[i, :])\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] /= normalization\n",
    "\n",
    "    for k in range(0, K):\n",
    "        normalization = sum(theta[k, :])\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Á¨¨‰∫åÊ≠•ÊòØÂØπ‰∫éÊñáÊ°£Êï∞ÊçÆÁöÑÂ§ÑÁêÜÔºåÊàë‰ª¨ÈúÄË¶ÅÂæóÂà∞ÁªüËÆ°Êï∞ÊçÆÔºå$n(d_i),n(d_i,w_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segmentation, stopwords filtering and document-word matrix generating\n",
    "# [return]:\n",
    "# N : number of documents\n",
    "# M : length of dictionary\n",
    "# word2id : a map mapping terms to their corresponding ids\n",
    "# id2word : a map mapping ids to terms\n",
    "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
    "def preprocessing(datasetFilePath, stopwordsFilePath):\n",
    "    \n",
    "    # read the stopwords file\n",
    "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
    "    stopwords = [line.strip() for line in file] \n",
    "    file.close()\n",
    "    \n",
    "    # read the documents\n",
    "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
    "    documents = [document.strip() for document in file] \n",
    "    file.close()\n",
    "\n",
    "    # number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    wordCounts = [];\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    currentId = 0;\n",
    "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
    "    for document in documents:\n",
    "        segList = jieba.cut(document)\n",
    "        wordCount = {}\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:               \n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = currentId;\n",
    "                    id2word[currentId] = word;\n",
    "                    currentId += 1;\n",
    "                if word in wordCount:\n",
    "                    wordCount[word] += 1\n",
    "                else:\n",
    "                    wordCount[word] = 1\n",
    "        wordCounts.append(wordCount);\n",
    "    \n",
    "    # length of dictionary\n",
    "    M = len(word2id)  \n",
    "\n",
    "    # generate the document-word matrix\n",
    "    X = zeros([N, M], int8)\n",
    "    for word in word2id.keys():\n",
    "        j = word2id[word]\n",
    "        for i in range(0, N):\n",
    "            if word in wordCounts[i]:\n",
    "                X[i, j] = wordCounts[i][word];    \n",
    "\n",
    "    return N, M, word2id, id2word, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the default params and read the params from cmd\n",
    "datasetFilePath = '../data/PLSA/dataset3.txt'\n",
    "stopwordsFilePath = '../data/PLSA/stopwords.dic'\n",
    "K = 10    # number of topic\n",
    "maxIteration = 30\n",
    "threshold = 10.0\n",
    "topicWordsNum = 10\n",
    "docTopicDist = '../data/PLSA/docTopicDistribution.txt'\n",
    "topicWordDist = '../data/PLSA/topicWordDistribution.txt'\n",
    "dictionary = '../data/PLSA/dictionary.dic'\n",
    "topicWords = '../data/PLSA/topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5757)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lamda[i, j] : p(zj|di)\n",
    "lamda = random([N, K])\n",
    "\n",
    "# theta[i, j] : p(wj|zi)\n",
    "theta = random([K, M])\n",
    "\n",
    "# p[i, j, k] : p(zk|di,wj)\n",
    "p = zeros([N, M, K])\n",
    "\n",
    "initializeParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Á¨¨‰∏âÊ≠•ÊòØÊàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆóEÔºåM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EStep():\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            denominator = 0;\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
    "                denominator += p[i, j, k];\n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] = 0;\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] /= denominator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MStep():\n",
    "    # update theta\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] = 0\n",
    "            for i in range(0, N):\n",
    "                theta[k, j] += X[i, j] * p[i, j, k]\n",
    "            denominator += theta[k, j]\n",
    "        if denominator == 0:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] = 1.0 / M\n",
    "        else:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] /= denominator\n",
    "        \n",
    "    # update lamda\n",
    "    for i in range(0, N):\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] = 0\n",
    "            denominator = 0\n",
    "            for j in range(0, M):\n",
    "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
    "                denominator += X[i, j];\n",
    "            if denominator == 0:\n",
    "                lamda[i, k] = 1.0 / K\n",
    "            else:\n",
    "                lamda[i, k] /= denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êé•ÁùÄÊàë‰ª¨ÂÆö‰πâÊúÄÂ§ß‰ººÁÑ∂ÂáΩÊï∞\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/36dda384ca41b649d5896500a3c6bdb63ecdcb64)\n",
    "Á¨¨‰∏ÄÈÉ®ÂàÜÊòØÂÆöÂÄºÔºåÊàë‰ª¨‰∏ªË¶Å‰ºòÂåñÁ¨¨‰∫åÈÉ®ÂàÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the log likelihood\n",
    "def LogLikelihood():\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            tmp = 0\n",
    "            for k in range(0, K):\n",
    "                tmp += theta[k, j] * lamda[i, k]\n",
    "            if tmp > 0:\n",
    "                loglikelihood += X[i, j] * log(tmp)\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# output the params of model and top words of topics to files\n",
    "def output():\n",
    "    # document-topic distribution\n",
    "    file = codecs.open(docTopicDist,'w','utf-8')\n",
    "    for i in range(0, N):\n",
    "        tmp = ''\n",
    "        for j in range(0, K):\n",
    "            tmp += str(lamda[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # topic-word distribution\n",
    "    file = codecs.open(topicWordDist,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        tmp = ''\n",
    "        for j in range(0, M):\n",
    "            tmp += str(theta[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # dictionary\n",
    "    file = codecs.open(dictionary,'w','utf-8')\n",
    "    for i in range(0, M):\n",
    "        file.write(id2word[i] + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # top words of each topic\n",
    "    file = codecs.open(topicWords,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        topicword = []\n",
    "        ids = theta[i, :].argsort()\n",
    "        for j in ids:\n",
    "            topicword.insert(0, id2word[j])\n",
    "        tmp = ''\n",
    "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
    "            tmp += word + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2017-10-11 15:34:36 ]  1  iteration   -119117.603574\n",
      "[ 2017-10-11 15:35:15 ]  2  iteration   -116686.457897\n",
      "[ 2017-10-11 15:35:55 ]  3  iteration   -113447.248935\n",
      "[ 2017-10-11 15:36:34 ]  4  iteration   -110041.623584\n",
      "[ 2017-10-11 15:37:14 ]  5  iteration   -107015.812203\n",
      "[ 2017-10-11 15:37:53 ]  6  iteration   -104487.345965\n",
      "[ 2017-10-11 15:38:33 ]  7  iteration   -102420.812678\n",
      "[ 2017-10-11 15:39:12 ]  8  iteration   -100820.586452\n",
      "[ 2017-10-11 15:39:52 ]  9  iteration   -99754.3630987\n",
      "[ 2017-10-11 15:40:31 ]  10  iteration   -99132.5837025\n",
      "[ 2017-10-11 15:41:10 ]  11  iteration   -98773.0945751\n",
      "[ 2017-10-11 15:41:50 ]  12  iteration   -98544.0665881\n",
      "[ 2017-10-11 15:42:33 ]  13  iteration   -98366.1570079\n",
      "[ 2017-10-11 15:43:12 ]  14  iteration   -98181.4161962\n",
      "[ 2017-10-11 15:43:52 ]  15  iteration   -97969.1043667\n",
      "[ 2017-10-11 15:44:31 ]  16  iteration   -97753.8946986\n",
      "[ 2017-10-11 15:45:10 ]  17  iteration   -97570.5593186\n",
      "[ 2017-10-11 15:45:50 ]  18  iteration   -97442.3102903\n",
      "[ 2017-10-11 15:46:29 ]  19  iteration   -97359.182354\n",
      "[ 2017-10-11 15:47:08 ]  20  iteration   -97299.9882036\n",
      "[ 2017-10-11 15:47:48 ]  21  iteration   -97257.0883117\n",
      "[ 2017-10-11 15:48:27 ]  22  iteration   -97220.6533569\n",
      "[ 2017-10-11 15:49:06 ]  23  iteration   -97189.8405976\n",
      "[ 2017-10-11 15:49:46 ]  24  iteration   -97168.4280998\n",
      "[ 2017-10-11 15:50:25 ]  25  iteration   -97150.2249148\n",
      "[ 2017-10-11 15:51:04 ]  26  iteration   -97135.9681279\n",
      "[ 2017-10-11 15:51:44 ]  27  iteration   -97118.4406008\n",
      "[ 2017-10-11 15:52:26 ]  28  iteration   -97104.7951924\n",
      "[ 2017-10-11 15:53:08 ]  29  iteration   -97099.3703603\n"
     ]
    }
   ],
   "source": [
    "# EM algorithm\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "for i in range(0, maxIteration):\n",
    "    EStep()\n",
    "    MStep()\n",
    "    newLoglikelihood = LogLikelihood()\n",
    "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood\n",
    "\n",
    "output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â≠©Â≠ê Á∫¢Ë°£ ÊΩòÂÖàÁîü Â≠óÊù° ‰∫§Ë≠¶ ÂëäËØâ Êú™ÊàêÂπ¥‰∫∫ ÊÄß‰æµ ÁΩëÂèã ËßÜÈ¢ë \r\n",
      "Êñ∞Êñ∞ Áî∑Â≠ê Ê∞ëË≠¶ Â≠©Â≠ê Â≠¶Ê†° ÁéãÊüê Áî∑Á´• Ë≠¶ÂÆòËØÅ ÂàòÂÖàÁîü ÈÖíÂêß \r\n",
      "Âë®Êù∞‰º¶ ËÆ∞ËÄÖ Âåó‰∫¨ ‰π¶ÁîªÈô¢ ÂæÆ‰ø° Ë≠¶Êñπ ÂÖ¨‰ºó Â´åÁñë‰∫∫ Ê∞ëË≠¶ ÁéãË∂Ö \r\n",
      "‰ΩïÂ§© Âè∏Êú∫ Èù¢ÂåÖËΩ¶ ÂâëÈîã Â∞èÂÅ∑ ÂÖ¨‰∫§ËΩ¶ ËÄÅ‰∫∫ ËæõÊüê ‰πòÂÆ¢ ËÆ∞ËÄÖ \r\n",
      "ËΩ¶ËæÜ È´òÈÄü È£ôËΩ¶ ‰∫ãÊïÖ ÂπøÂ∑û ÂèëÁîü ‰∫§Ë≠¶ ÂàÜÁ∫¢ ËÆ∞ËÄÖ Ë°åÈ©∂ \r\n",
      "ÂàÜÁ∫¢ ÊùëÊ∞ë Èì∂Ë°å Ê∞ëË≠¶ ËÖæÂÜ≤ Â©ÜÂ©Ü Â§ßÂ¢© ÂàòÊüê È°∫Âæ∑ ÂúüË±™ \r\n",
      "Â≠©Â≠ê ÂÆ∂Èïø Ê±ÇÂ©ö ÊùéÂ•≥Â£´ ÂÑøÂ≠ê ÊúãÂèã ‰∫§Ë≠¶ ÊïôËÇ≤ Â∞èËãè ËÄÅÂ∏à \r\n",
      "ÊâãÊú∫ ÊôØÂå∫ Ê∏∏ÂÆ¢ ÈæôÊ±† ÊïëÊè¥ ÊùëÊ∞ë ÊêúÊïë ËÉ°Êïè ÊïëÊè¥Èòü ÁôªÂ±± \r\n",
      "Êä•Ë≠¶ Ê∞ëË≠¶ È©¨ËµõÂÖã Ë≠¶Êñπ ÂæÆÂçö Â´åÁäØ Êã®Êâì Ê≤ªÁñó ÁÖßÁâá Ë°åÊîøÊãòÁïô \r\n",
      "ÂàòÂ∞ß Ê≤≥Ê∫ê ÂÅáÂ∏Å ËÆ∞ËÄÖ Ë≠¶Êñπ Áâ©ÊµÅ Â•≥Â£´ ‰∏æÊä• Â∏¶Ëµ∞ ÂåªÈô¢ \r\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/PLSA/topics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "‰∏äÈù¢ÁöÑÊúçÂä°‰ΩøÁî®Ëµ∑Êù•ÁâπÂà´ÁöÑÊÖ¢„ÄÇ„ÄÇ‰ΩøÁî®\n",
    "\n",
    "‰∏ãÈù¢Êàë‰ΩøÁî®gensim‰∏≠ÁöÑplsaÊñπÊ≥ïÔºåÂú®gensim‰∏≠Âπ∂Ê≤°Êúâ‰∏ìÈó®ÂÆûÁé∞plsaÔºåÂè™ÊúâLDAÔºåÈÄöËøáËÆæÁΩÆLDAÂèÇÊï∞ÂèØ‰ª•Êù•ÂÆûÁé∞plsaÔºåÊâÄÊúâÊàë‰ª¨ÂÖàÊù•ÁúãLDAÁöÑ„ÄÇ\n",
    "\n",
    "### 3. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Áã¨Á´ãÂêåÂàÜÂ∏É independent and identically distributed (i.i.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log to external log file\n",
    "# import logging\n",
    "# logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set up log to terminal\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "短语挖掘方法介绍\n",
    "\n",
    "第一大类：Unigram topic modeling 一元的主题模型，主要包括：\n",
    "\n",
    "1. Latent Semantic Analysis (LSA)\n",
    "2. Probablistic Latent Semantic Analysis （pLSA）\n",
    "3. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "下面开始对这几种模型的介绍\n",
    "\n",
    "### 1.  LSA\n",
    "参考博文[关于 LSA（Latent Semantic Analysis）主题模型的个人理解](http://blog.csdn.net/cang_sheng_ta_ge/article/details/46708515)，另外一篇英文文章[Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "\n",
    "其主要思想感觉有点类似于词嵌入，我们假设有好多词，那可以将文档用词来进行one-hot encoding，但是这样会造成文章大量的稀疏表示，于是我们想法就是怎么能够将稀疏矩阵变为稠密矩阵，一个做法就是矩阵分解。\n",
    "\n",
    "下面代码参考 https://github.com/chrisjmccormick/LSA_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处的数据使用的是路透社的数据，大概类别有100左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n"
     ]
    }
   ],
   "source": [
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 LSA 第一步就是就按tf-idf值，我们直接使用 sklearn 中的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们去除了停用词\n",
    "# 过滤掉出现文档超过50%的词\n",
    "# 过滤掉最小出现2次的词\n",
    "# 挑选出前1w个词\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: 10000\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4743, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步我们要做的就是对高维的特征矩阵进行分解，SVD奇异值分解。\n",
    "\n",
    "X_train_tfidf 中每一行代表一篇文章，每一列代表一个词，每个数值都是tf-idf值。\n",
    "\n",
    "奇异值分解做的事情就是将高纬度稀疏矩阵分解为3个矩阵连乘。网上找到一篇非常好的文章：[奇异值分解 (SVD) --- 几何意义](http://blog.sciencenet.cn/blog-696950-699432.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先对训练数据求取svd，然后对齐进行正则化，得到单位向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到转换后的特征值，并且这些特征占总的信息量为27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Explained variance of the SVD step: 27%\n"
     ]
    }
   ],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始对文章进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4471 / 4858) correct - 92.03%\n",
      "  done in 1.404sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4561 / 4858) correct - 93.89%\n",
      "    done in 0.749sec\n"
     ]
    }
   ],
   "source": [
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(X_test_lsa)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim使用\n",
    "使用 gensim 来做 LSA 的分析，Latent Semantic Indexing (also known as Latent Semantic Analysis)，在 gensim 中对应的模块是models.lsimodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTfidfCorups():\n",
    "    import pickle\n",
    "    raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "    X_train_raw = raw_text_dataset[0]\n",
    "    y_train_labels = raw_text_dataset[1] \n",
    "    X_test_raw = raw_text_dataset[2]\n",
    "    y_test_labels = raw_text_dataset[3]\n",
    "    \n",
    "    # Tokenize the documents.\n",
    "\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "    def docTokenizer(docs):\n",
    "        # Split the documents into tokens.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        newdocs = []\n",
    "        for idx in range(len(docs)):\n",
    "            doc = docs[idx].lower()  # Convert to lowercase.\n",
    "            newdocs.append(tokenizer.tokenize(doc))  # Split into words.\n",
    "\n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        newdocs = [[token for token in doc if not token.isnumeric()] for doc in newdocs]\n",
    "\n",
    "        # Remove words that are only one character.\n",
    "        newdocs = [[token for token in doc if len(token) > 1] for doc in newdocs]\n",
    "\n",
    "        return newdocs\n",
    "    \n",
    "    x_train_tokens = docTokenizer(X_train_raw)\n",
    "    x_test_tokens = docTokenizer(X_test_raw)\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    x_train_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_train_tokens]\n",
    "    x_test_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_test_tokens]\n",
    "    \n",
    "    # Remove rare and common tokens.\n",
    "    import numpy as np\n",
    "    from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(np.concatenate((x_train_tokens,x_test_tokens)))\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "    \n",
    "    from gensim.models import TfidfModel\n",
    "    # Vectorize data.\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corups = [dictionary.doc2bow(doc) for doc in np.concatenate((x_train_tokens,x_test_tokens))]\n",
    "    \n",
    "    corpus_tfidf = tfidf[corups]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "整个处理过程可以分为4步：\n",
    "\n",
    "1. 将文档分词\n",
    "2. 单词Lemmatize\n",
    "3. idf值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docTokenizer(docs):\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    newdocs = []\n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx].lower()  # Convert to lowercase.\n",
    "        newdocs.append(tokenizer.tokenize(doc))  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    newdocs = [[token for token in doc if not token.isnumeric()] for doc in newdocs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    newdocs = [[token for token in doc if len(token) > 1] for doc in newdocs]\n",
    "    \n",
    "    return newdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_tokens = docTokenizer(X_train_raw)\n",
    "x_test_tokens = docTokenizer(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "x_train_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_train_tokens]\n",
    "x_test_tokens = [[lemmatizer.lemmatize(token) for token in doc] for doc in x_test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步我们要做的就是建立corups了，去除超过在一半文档都出现的词，去除出现文档数小于20的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:24:20,453 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-09-30 16:24:22,217 : INFO : built Dictionary(27466 unique tokens: ['bahia', 'cocoa', 'review', 'shower', 'continued']...) from 9601 documents (total 1214137 corpus positions)\n",
      "2017-09-30 16:24:22,279 : INFO : discarding 24027 tokens: [('bahia', 1), ('shower', 6), ('the', 7371), ('in', 6381), ('alleviating', 2), ('and', 6869), ('for', 5439), ('temporao', 2), ('humidity', 3), ('restored', 18)]...\n",
      "2017-09-30 16:24:22,281 : INFO : keeping 3439 tokens which were in no less than 20 and no more than 4800 (=50.0%) documents\n",
      "2017-09-30 16:24:22,293 : INFO : resulting dictionary: Dictionary(3439 unique tokens: ['cocoa', 'review', 'continued', 'throughout', 'week']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(np.concatenate((x_train_tokens,x_test_tokens)))\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处去除完后，字典就剩 3439 了，下面我们处理语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "# Vectorize data.\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "# Bag-of-words representation of the documents.\n",
    "corups = [dictionary.doc2bow(doc) for doc in np.concatenate((x_train_tokens,x_test_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel,TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 16:42:37,165 : INFO : using serial LSI version on this node\n",
      "2017-09-30 16:42:37,167 : INFO : updating model with new documents\n",
      "2017-09-30 16:42:38,488 : INFO : preparing a new chunk of documents\n",
      "2017-09-30 16:42:38,674 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-09-30 16:42:38,676 : INFO : 1st phase: constructing (3439, 200) action matrix\n",
      "2017-09-30 16:42:38,922 : INFO : orthonormalizing (3439, 200) action matrix\n",
      "2017-09-30 16:42:39,723 : INFO : 2nd phase: running dense svd on (200, 9601) matrix\n",
      "2017-09-30 16:42:39,900 : INFO : computing the final decomposition\n",
      "2017-09-30 16:42:39,903 : INFO : keeping 100 factors (discarding 20.809% of energy spectrum)\n",
      "2017-09-30 16:42:39,909 : INFO : processed documents up to #9601\n",
      "2017-09-30 16:42:39,912 : INFO : topic #0(24.459): -0.640*\"v\" + -0.364*\"ct\" + -0.289*\"net\" + -0.261*\"loss\" + -0.228*\"shr\" + -0.196*\"mln\" + -0.173*\"rev\" + -0.155*\"profit\" + -0.142*\"qtr\" + -0.098*\"oper\"\n",
      "2017-09-30 16:42:39,914 : INFO : topic #1(16.559): 0.180*\"pct\" + -0.173*\"v\" + 0.163*\"bank\" + 0.140*\"share\" + 0.136*\"billion\" + 0.122*\"dlrs\" + 0.113*\"will\" + 0.110*\"he\" + 0.110*\"is\" + 0.108*\"be\"\n",
      "2017-09-30 16:42:39,916 : INFO : topic #2(13.214): -0.386*\"qtly\" + -0.366*\"div\" + -0.330*\"ct\" + -0.301*\"record\" + -0.268*\"prior\" + -0.262*\"pay\" + -0.255*\"april\" + 0.217*\"loss\" + -0.185*\"dividend\" + -0.180*\"quarterly\"\n",
      "2017-09-30 16:42:39,919 : INFO : topic #3(9.648): 0.368*\"share\" + -0.217*\"billion\" + 0.212*\"offering\" + 0.195*\"stock\" + 0.182*\"common\" + 0.151*\"inc\" + -0.150*\"bank\" + 0.129*\"debenture\" + 0.123*\"convertible\" + 0.122*\"company\"\n",
      "2017-09-30 16:42:39,921 : INFO : topic #4(9.361): -0.794*\"loss\" + -0.294*\"profit\" + 0.244*\"v\" + 0.176*\"mln\" + 0.165*\"net\" + 0.091*\"billion\" + -0.086*\"oper\" + -0.083*\"qtly\" + 0.082*\"pct\" + 0.077*\"shr\"\n"
     ]
    }
   ],
   "source": [
    "lsi = LsiModel(corpus_tfidf, num_topics=100,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [[item[1] for item in vec] for vec in  lsi[corpus_tfidf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858 9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw),len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4593 / 4858) correct - 94.55%\n",
      "  done in 0.699sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(vectors[:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(vectors[4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的预处理，我们模型的预测结果又有了提升 😂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA 的延伸\n",
    "看到上面 LSA 方法的时候，其实我就想到了 word embeding，那我们能够用神经网络来做LSA嘛，思路：我们将document进行embeding，然后将词也进行embedding，然后经过一个3层全连接，输出词在document中出现的次数，我们不断的拟合上面的网络，就能得到 document 和 word 的向量了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9601\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw)+len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInput = keras.Input((1,),name='word_input',dtype='int32')\n",
    "docInput = keras.Input((1,),name='doc_input',dtype='int32')\n",
    "# countInput = keras.Input((1,),name='count_input',dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_topic_num = 100\n",
    "wordEmbed = keras.layers.embeddings.Embedding(10000,latent_topic_num,input_length=1,\n",
    "                                              embeddings_initializer=keras.initializers.VarianceScaling(2),name='word_embed')\n",
    "docEmbed = keras.layers.embeddings.Embedding(9601,latent_topic_num,input_length=1,\n",
    "                                  embeddings_initializer=keras.initializers.VarianceScaling(2),name='doc_embed')\n",
    "word_embed = keras.layers.Reshape((latent_topic_num,))(wordEmbed(wordInput)) # can also use Flatten\n",
    "doc_embed = keras.layers.Reshape((latent_topic_num,))(docEmbed(docInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_16/Reshape:0' shape=(?, 100) dtype=float32>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vector = keras.layers.concatenate([word_embed,doc_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_9/concat:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_len = 3\n",
    "# layer_num = 10\n",
    "layer_nums = [20,10,5]\n",
    "for i in range(len(layer_nums)):\n",
    "    input_vector = keras.layers.Dense(layer_nums[i],\n",
    "                                      kernel_initializer=keras.initializers.VarianceScaling(2),\n",
    "                                     activation='relu')(input_vector)\n",
    "    \n",
    "y_predict = keras.layers.Dense(1,kernel_initializer=keras.initializers.VarianceScaling(2))(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model([wordInput,docInput],y_predict)\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面根据上面的模型，我们来定制我们的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_vectorizer = CountVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_documents = np.concatenate((X_train_raw,X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_tf = dnn_vectorizer.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 10000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_frequence = doc_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInputs = []\n",
    "docInputs = []\n",
    "targetCounts = []\n",
    "for i in range(len(term_frequence)):\n",
    "    doc_frequence = term_frequence[i]\n",
    "    for j in range(len(doc_frequence)):\n",
    "        if doc_frequence[j] != 0:\n",
    "            docInputs.append(i)\n",
    "            wordInputs.append(j)\n",
    "            targetCounts.append(doc_frequence[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "doc_input (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word_embed (Embedding)           (None, 1, 5)          48005       word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "doc_embed (Embedding)            (None, 1, 5)          50000       doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)             (None, 5)             0           word_embed[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)             (None, 5)             0           doc_embed[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 10)            0           reshape_12[0][0]                 \n",
      "                                                                   reshape_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 20)            220         concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 10)            210         dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 5)             55          dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 1)             6           dense_26[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 98,496\n",
      "Trainable params: 98,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498923"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3101     \n",
      "Epoch 2/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3077     \n",
      "Epoch 3/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3062     \n",
      "Epoch 4/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3030     \n",
      "Epoch 5/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.3010     \n",
      "Epoch 6/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2990     \n",
      "Epoch 7/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2963     \n",
      "Epoch 8/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2961     \n",
      "Epoch 9/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2949     \n",
      "Epoch 10/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2915     \n",
      "Epoch 11/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2895     \n",
      "Epoch 12/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2881     \n",
      "Epoch 13/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2870     \n",
      "Epoch 14/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2853     \n",
      "Epoch 15/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2831     \n",
      "Epoch 16/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2817     \n",
      "Epoch 17/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2801     \n",
      "Epoch 18/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2795     \n",
      "Epoch 19/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2769     \n",
      "Epoch 20/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2756     \n",
      "Epoch 21/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2743     \n",
      "Epoch 22/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2731     \n",
      "Epoch 23/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2715     \n",
      "Epoch 24/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2701     \n",
      "Epoch 25/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2692     \n",
      "Epoch 26/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2668     \n",
      "Epoch 27/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2654     \n",
      "Epoch 28/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2647     \n",
      "Epoch 29/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2639     \n",
      "Epoch 30/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2622     \n",
      "Epoch 31/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2613     \n",
      "Epoch 32/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2605     \n",
      "Epoch 33/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2592     \n",
      "Epoch 34/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2590     \n",
      "Epoch 35/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2578     \n",
      "Epoch 36/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2568     \n",
      "Epoch 37/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2574     \n",
      "Epoch 38/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2551     \n",
      "Epoch 39/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2530     \n",
      "Epoch 40/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2516     \n",
      "Epoch 41/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2502     \n",
      "Epoch 42/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2507     \n",
      "Epoch 43/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2484     \n",
      "Epoch 44/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2472     \n",
      "Epoch 45/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2469     \n",
      "Epoch 46/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2463     \n",
      "Epoch 47/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2448     \n",
      "Epoch 48/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2433     \n",
      "Epoch 49/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2420     \n",
      "Epoch 50/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2412     \n",
      "Epoch 51/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2418     \n",
      "Epoch 52/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2407     \n",
      "Epoch 53/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2402     \n",
      "Epoch 54/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2400     \n",
      "Epoch 55/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2397     \n",
      "Epoch 56/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2384     \n",
      "Epoch 57/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2368     \n",
      "Epoch 58/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2353     \n",
      "Epoch 59/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2346     \n",
      "Epoch 60/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2342     \n",
      "Epoch 61/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2339     \n",
      "Epoch 62/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2317     \n",
      "Epoch 63/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2322     \n",
      "Epoch 64/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2314     \n",
      "Epoch 65/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2307     \n",
      "Epoch 66/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2303     \n",
      "Epoch 67/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2289     \n",
      "Epoch 68/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2286     \n",
      "Epoch 69/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2277     \n",
      "Epoch 70/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2263     \n",
      "Epoch 71/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2261     \n",
      "Epoch 72/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2256     \n",
      "Epoch 73/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2254     \n",
      "Epoch 74/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2239     \n",
      "Epoch 75/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2241     \n",
      "Epoch 76/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2231     \n",
      "Epoch 77/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2224     \n",
      "Epoch 78/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2212     \n",
      "Epoch 79/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2199     \n",
      "Epoch 80/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2204     \n",
      "Epoch 81/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2207     \n",
      "Epoch 82/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2190     \n",
      "Epoch 83/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2185     \n",
      "Epoch 84/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2184     \n",
      "Epoch 85/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2177     \n",
      "Epoch 86/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2169     \n",
      "Epoch 87/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2163     \n",
      "Epoch 88/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2167     \n",
      "Epoch 89/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2155     \n",
      "Epoch 90/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2152     \n",
      "Epoch 91/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2145     \n",
      "Epoch 92/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2148     \n",
      "Epoch 93/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2139     \n",
      "Epoch 94/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2133     \n",
      "Epoch 95/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2128     \n",
      "Epoch 96/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2130     \n",
      "Epoch 97/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2118     \n",
      "Epoch 98/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2117     \n",
      "Epoch 99/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2106     \n",
      "Epoch 100/100\n",
      "498923/498923 [==============================] - 2s - loss: 0.2095     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d1079ef28>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array(wordInputs),np.array(docInputs)],np.array(targetCounts),batch_size=10000,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 5)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743 4858\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_raw),len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (4188 / 4858) correct - 86.21%\n",
      "    done in 0.515sec\n"
     ]
    }
   ],
   "source": [
    "#  (4183 / 4858) correct - 86.11%,\n",
    "#  0.4882 - 85.78% \n",
    "# 100个hidden，100次，loss:0.3117 , correct - 86.39%\n",
    "# 100个hidden，100次，loss:0.2095 , correct - 86.21%\n",
    "# 可以说上面这种做法\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(model.get_weights()[1][:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(model.get_weights()[1][4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：通过上面的dnn方法，baseline的model效果并不是很好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pLSA\n",
    "在LSA中，我们构建了term和document的共现矩阵，\n",
    "![图片](http://bos.nj.bpc.baidu.com/v1/agroup/afed8760421ed8b2b3d003e9b14f7f56673cc4a3)\n",
    "这个的核心假设是：使用词袋模型来表示文章（即使丢失了term的顺序）也能很好的代表文章信息，LSA 通过svg方法，将document映射到一个隐向量空间上去。\n",
    "\n",
    "我们更进一步，考虑引入一个隐含变量$z_k$，来表示文档主题，形成简单的贝叶斯网络，\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/f67618811933d48654161ae53fe5f2d836ace8d8)\n",
    "\n",
    "非常好的文档可以参考：\n",
    "\n",
    "[概率语言模型及其变形系列 (1)-PLSA 及 EM 算法](http://blog.csdn.net/yangliuy/article/details/8330640)\n",
    "\n",
    "[自然语言处理之 PLSA](http://zhikaizhang.cn/2016/06/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8BPLSA/)\n",
    "\n",
    "[主题模型之 pLSA](http://blog.jqian.net/post/plsa.html)\n",
    "\n",
    "里面重要的点是EM的思想，即先假设参数$\\theta$已知，然后求隐变量的后验概率，然后此时知道隐变量后，就可以和数据X一起组成完整的数据集，此时再来求极值。\n",
    "\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/4a83a5cf5beaced4dbe535759365c1bab4b2862b)\n",
    "\n",
    "下面是具体的算法实现，代码参考自：https://github.com/laserwave/PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import sys\n",
    "import jieba\n",
    "import re\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "代码中使用 $lamda[i,k]$ 表示参数 $p(z_k|d_i)$, 用 $theta[k,j]$ 表示参数 $p(w_j|z_k)$, 用 $p[i,j,k]$ 表示隐藏变量的后验概率 $p(z_k|d_i,w_j)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "第一步，我们需要初始化参数 $p(z_k|d_i)$, $p(w_j|z_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeParameters():\n",
    "    for i in range(0, N):\n",
    "        normalization = sum(lamda[i, :])\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] /= normalization\n",
    "\n",
    "    for k in range(0, K):\n",
    "        normalization = sum(theta[k, :])\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步是对于文档数据的处理，我们需要得到统计数据，$n(d_i),n(d_i,w_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segmentation, stopwords filtering and document-word matrix generating\n",
    "# [return]:\n",
    "# N : number of documents\n",
    "# M : length of dictionary\n",
    "# word2id : a map mapping terms to their corresponding ids\n",
    "# id2word : a map mapping ids to terms\n",
    "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
    "def preprocessing(datasetFilePath, stopwordsFilePath):\n",
    "    \n",
    "    # read the stopwords file\n",
    "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
    "    stopwords = [line.strip() for line in file] \n",
    "    file.close()\n",
    "    \n",
    "    # read the documents\n",
    "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
    "    documents = [document.strip() for document in file] \n",
    "    file.close()\n",
    "\n",
    "    # number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    wordCounts = [];\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    currentId = 0;\n",
    "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
    "    for document in documents:\n",
    "        segList = jieba.cut(document)\n",
    "        wordCount = {}\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:               \n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = currentId;\n",
    "                    id2word[currentId] = word;\n",
    "                    currentId += 1;\n",
    "                if word in wordCount:\n",
    "                    wordCount[word] += 1\n",
    "                else:\n",
    "                    wordCount[word] = 1\n",
    "        wordCounts.append(wordCount);\n",
    "    \n",
    "    # length of dictionary\n",
    "    M = len(word2id)  \n",
    "\n",
    "    # generate the document-word matrix\n",
    "    X = zeros([N, M], int8)\n",
    "    for word in word2id.keys():\n",
    "        j = word2id[word]\n",
    "        for i in range(0, N):\n",
    "            if word in wordCounts[i]:\n",
    "                X[i, j] = wordCounts[i][word];    \n",
    "\n",
    "    return N, M, word2id, id2word, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the default params and read the params from cmd\n",
    "datasetFilePath = '../data/PLSA/dataset3.txt'\n",
    "stopwordsFilePath = '../data/PLSA/stopwords.dic'\n",
    "K = 10    # number of topic\n",
    "maxIteration = 30\n",
    "threshold = 10.0\n",
    "topicWordsNum = 10\n",
    "docTopicDist = '../data/PLSA/docTopicDistribution.txt'\n",
    "topicWordDist = '../data/PLSA/topicWordDistribution.txt'\n",
    "dictionary = '../data/PLSA/dictionary.dic'\n",
    "topicWords = '../data/PLSA/topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5757)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lamda[i, j] : p(zj|di)\n",
    "lamda = random([N, K])\n",
    "\n",
    "# theta[i, j] : p(wj|zi)\n",
    "theta = random([K, M])\n",
    "\n",
    "# p[i, j, k] : p(zk|di,wj)\n",
    "p = zeros([N, M, K])\n",
    "\n",
    "initializeParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步是我们需要计算E，M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EStep():\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            denominator = 0;\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
    "                denominator += p[i, j, k];\n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] = 0;\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] /= denominator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MStep():\n",
    "    # update theta\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] = 0\n",
    "            for i in range(0, N):\n",
    "                theta[k, j] += X[i, j] * p[i, j, k]\n",
    "            denominator += theta[k, j]\n",
    "        if denominator == 0:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] = 1.0 / M\n",
    "        else:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] /= denominator\n",
    "        \n",
    "    # update lamda\n",
    "    for i in range(0, N):\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] = 0\n",
    "            denominator = 0\n",
    "            for j in range(0, M):\n",
    "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
    "                denominator += X[i, j];\n",
    "            if denominator == 0:\n",
    "                lamda[i, k] = 1.0 / K\n",
    "            else:\n",
    "                lamda[i, k] /= denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们定义最大似然函数\n",
    "\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/36dda384ca41b649d5896500a3c6bdb63ecdcb64)\n",
    "第一部分是定值，我们主要优化第二部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the log likelihood\n",
    "def LogLikelihood():\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            tmp = 0\n",
    "            for k in range(0, K):\n",
    "                tmp += theta[k, j] * lamda[i, k]\n",
    "            if tmp > 0:\n",
    "                loglikelihood += X[i, j] * log(tmp)\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# output the params of model and top words of topics to files\n",
    "def output():\n",
    "    # document-topic distribution\n",
    "    file = codecs.open(docTopicDist,'w','utf-8')\n",
    "    for i in range(0, N):\n",
    "        tmp = ''\n",
    "        for j in range(0, K):\n",
    "            tmp += str(lamda[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # topic-word distribution\n",
    "    file = codecs.open(topicWordDist,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        tmp = ''\n",
    "        for j in range(0, M):\n",
    "            tmp += str(theta[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # dictionary\n",
    "    file = codecs.open(dictionary,'w','utf-8')\n",
    "    for i in range(0, M):\n",
    "        file.write(id2word[i] + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # top words of each topic\n",
    "    file = codecs.open(topicWords,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        topicword = []\n",
    "        ids = theta[i, :].argsort()\n",
    "        for j in ids:\n",
    "            topicword.insert(0, id2word[j])\n",
    "        tmp = ''\n",
    "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
    "            tmp += word + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2017-10-11 15:34:36 ]  1  iteration   -119117.603574\n",
      "[ 2017-10-11 15:35:15 ]  2  iteration   -116686.457897\n",
      "[ 2017-10-11 15:35:55 ]  3  iteration   -113447.248935\n",
      "[ 2017-10-11 15:36:34 ]  4  iteration   -110041.623584\n",
      "[ 2017-10-11 15:37:14 ]  5  iteration   -107015.812203\n",
      "[ 2017-10-11 15:37:53 ]  6  iteration   -104487.345965\n",
      "[ 2017-10-11 15:38:33 ]  7  iteration   -102420.812678\n",
      "[ 2017-10-11 15:39:12 ]  8  iteration   -100820.586452\n",
      "[ 2017-10-11 15:39:52 ]  9  iteration   -99754.3630987\n",
      "[ 2017-10-11 15:40:31 ]  10  iteration   -99132.5837025\n",
      "[ 2017-10-11 15:41:10 ]  11  iteration   -98773.0945751\n",
      "[ 2017-10-11 15:41:50 ]  12  iteration   -98544.0665881\n",
      "[ 2017-10-11 15:42:33 ]  13  iteration   -98366.1570079\n",
      "[ 2017-10-11 15:43:12 ]  14  iteration   -98181.4161962\n",
      "[ 2017-10-11 15:43:52 ]  15  iteration   -97969.1043667\n",
      "[ 2017-10-11 15:44:31 ]  16  iteration   -97753.8946986\n",
      "[ 2017-10-11 15:45:10 ]  17  iteration   -97570.5593186\n",
      "[ 2017-10-11 15:45:50 ]  18  iteration   -97442.3102903\n",
      "[ 2017-10-11 15:46:29 ]  19  iteration   -97359.182354\n",
      "[ 2017-10-11 15:47:08 ]  20  iteration   -97299.9882036\n",
      "[ 2017-10-11 15:47:48 ]  21  iteration   -97257.0883117\n",
      "[ 2017-10-11 15:48:27 ]  22  iteration   -97220.6533569\n",
      "[ 2017-10-11 15:49:06 ]  23  iteration   -97189.8405976\n",
      "[ 2017-10-11 15:49:46 ]  24  iteration   -97168.4280998\n",
      "[ 2017-10-11 15:50:25 ]  25  iteration   -97150.2249148\n",
      "[ 2017-10-11 15:51:04 ]  26  iteration   -97135.9681279\n",
      "[ 2017-10-11 15:51:44 ]  27  iteration   -97118.4406008\n",
      "[ 2017-10-11 15:52:26 ]  28  iteration   -97104.7951924\n",
      "[ 2017-10-11 15:53:08 ]  29  iteration   -97099.3703603\n"
     ]
    }
   ],
   "source": [
    "# EM algorithm\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "for i in range(0, maxIteration):\n",
    "    EStep()\n",
    "    MStep()\n",
    "    newLoglikelihood = LogLikelihood()\n",
    "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood\n",
    "\n",
    "output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孩子 红衣 潘先生 字条 交警 告诉 未成年人 性侵 网友 视频 \r\n",
      "新新 男子 民警 孩子 学校 王某 男童 警官证 刘先生 酒吧 \r\n",
      "周杰伦 记者 北京 书画院 微信 警方 公众 嫌疑人 民警 王超 \r\n",
      "何天 司机 面包车 剑锋 小偷 公交车 老人 辛某 乘客 记者 \r\n",
      "车辆 高速 飙车 事故 广州 发生 交警 分红 记者 行驶 \r\n",
      "分红 村民 银行 民警 腾冲 婆婆 大墩 刘某 顺德 土豪 \r\n",
      "孩子 家长 求婚 李女士 儿子 朋友 交警 教育 小苏 老师 \r\n",
      "手机 景区 游客 龙池 救援 村民 搜救 胡敏 救援队 登山 \r\n",
      "报警 民警 马赛克 警方 微博 嫌犯 拨打 治疗 照片 行政拘留 \r\n",
      "刘尧 河源 假币 记者 警方 物流 女士 举报 带走 医院 \r\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/PLSA/topics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上面的服务使用起来特别的慢。。使用\n",
    "\n",
    "下面我使用gensim中的plsa方法，在gensim中并没有专门实现plsa，只有LDA，通过设置LDA参数可以来实现plsa，所有我们先来看LDA的。\n",
    "\n",
    "### 3. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "独立同分布 independent and identically distributed (i.i.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log to external log file\n",
    "# import logging\n",
    "# logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set up log to terminal\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-12 16:23:20,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-10-12 16:23:22,581 : INFO : built Dictionary(27466 unique tokens: ['bahia', 'cocoa', 'review', 'shower', 'continued']...) from 9601 documents (total 1214137 corpus positions)\n",
      "2017-10-12 16:23:22,654 : INFO : discarding 24027 tokens: [('bahia', 1), ('shower', 6), ('the', 7371), ('in', 6381), ('alleviating', 2), ('and', 6869), ('for', 5439), ('temporao', 2), ('humidity', 3), ('restored', 18)]...\n",
      "2017-10-12 16:23:22,655 : INFO : keeping 3439 tokens which were in no less than 20 and no more than 4800 (=50.0%) documents\n",
      "2017-10-12 16:23:22,668 : INFO : resulting dictionary: Dictionary(3439 unique tokens: ['cocoa', 'review', 'continued', 'throughout', 'week']...)\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, dictionary =  getTfidfCorups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-12 16:23:40,912 : INFO : using symmetric alpha at 0.01\n",
      "2017-10-12 16:23:40,913 : INFO : using symmetric eta at 0.0002907822041291073\n",
      "2017-10-12 16:23:40,915 : INFO : using serial LDA version on this node\n",
      "2017-10-12 16:23:46,279 : INFO : running online (single-pass) LDA training, 100 topics, 1 passes over the supplied corpus of 9601 documents, updating model once every 2000 documents, evaluating perplexity every 9601 documents, iterating 500x with a convergence threshold of 0.001000\n",
      "2017-10-12 16:23:46,281 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2017-10-12 16:23:46,487 : INFO : PROGRESS: pass 0, at document #2000/9601\n",
      "2017-10-12 16:23:52,232 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:23:52,882 : INFO : topic #99 (0.010): 0.013*\"v\" + 0.009*\"mln\" + 0.007*\"ct\" + 0.007*\"ford\" + 0.007*\"net\" + 0.006*\"sale\" + 0.006*\"ltd\" + 0.005*\"pct\" + 0.005*\"shr\" + 0.005*\"split\"\n",
      "2017-10-12 16:23:52,884 : INFO : topic #38 (0.010): 0.018*\"v\" + 0.010*\"march\" + 0.010*\"ct\" + 0.008*\"sale\" + 0.007*\"billion\" + 0.007*\"store\" + 0.007*\"president\" + 0.006*\"dlrs\" + 0.006*\"shr\" + 0.006*\"foot\"\n",
      "2017-10-12 16:23:52,885 : INFO : topic #88 (0.010): 0.010*\"v\" + 0.009*\"ct\" + 0.006*\"tax\" + 0.006*\"group\" + 0.006*\"mart\" + 0.006*\"offer\" + 0.006*\"grant\" + 0.005*\"coffee\" + 0.005*\"pct\" + 0.005*\"center\"\n",
      "2017-10-12 16:23:52,887 : INFO : topic #91 (0.010): 0.016*\"reject\" + 0.013*\"hotel\" + 0.012*\"gatt\" + 0.012*\"store\" + 0.011*\"pct\" + 0.010*\"franklin\" + 0.009*\"offering\" + 0.009*\"insured\" + 0.009*\"productivity\" + 0.009*\"ohio\"\n",
      "2017-10-12 16:23:52,888 : INFO : topic #24 (0.010): 0.028*\"v\" + 0.014*\"net\" + 0.013*\"ct\" + 0.012*\"mln\" + 0.011*\"rev\" + 0.010*\"shr\" + 0.009*\"dlrs\" + 0.007*\"bbl\" + 0.006*\"tonne\" + 0.005*\"4th\"\n",
      "2017-10-12 16:23:52,891 : INFO : topic diff=78.508443, rho=1.000000\n",
      "2017-10-12 16:23:53,099 : INFO : PROGRESS: pass 0, at document #4000/9601\n",
      "2017-10-12 16:23:59,346 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:23:59,833 : INFO : topic #83 (0.010): 0.025*\"coin\" + 0.020*\"billion\" + 0.019*\"fund\" + 0.019*\"loan\" + 0.018*\"versus\" + 0.018*\"loss\" + 0.018*\"asset\" + 0.018*\"dlrs\" + 0.015*\"funding\" + 0.014*\"latest\"\n",
      "2017-10-12 16:23:59,835 : INFO : topic #89 (0.010): 0.014*\"hughes\" + 0.014*\"billion\" + 0.012*\"split\" + 0.010*\"declares\" + 0.010*\"baker\" + 0.009*\"authorized\" + 0.009*\"treasury\" + 0.009*\"appropriate\" + 0.008*\"pct\" + 0.008*\"dlrs\"\n",
      "2017-10-12 16:23:59,836 : INFO : topic #80 (0.010): 0.016*\"ecuador\" + 0.009*\"export\" + 0.009*\"rate\" + 0.008*\"v\" + 0.008*\"profit\" + 0.008*\"treasury\" + 0.007*\"crown\" + 0.007*\"bill\" + 0.007*\"tonne\" + 0.007*\"dollar\"\n",
      "2017-10-12 16:23:59,837 : INFO : topic #60 (0.010): 0.019*\"v\" + 0.011*\"guilder\" + 0.011*\"share\" + 0.009*\"net\" + 0.009*\"ct\" + 0.009*\"completed\" + 0.008*\"sale\" + 0.008*\"ag\" + 0.008*\"common\" + 0.008*\"mln\"\n",
      "2017-10-12 16:23:59,838 : INFO : topic #30 (0.010): 0.016*\"canada\" + 0.015*\"bond\" + 0.012*\"king\" + 0.011*\"canadian\" + 0.009*\"cyclops\" + 0.008*\"conversion\" + 0.008*\"dlrs\" + 0.007*\"issue\" + 0.007*\"v\" + 0.007*\"group\"\n",
      "2017-10-12 16:23:59,840 : INFO : topic diff=0.923743, rho=0.707107\n",
      "2017-10-12 16:24:00,053 : INFO : PROGRESS: pass 0, at document #6000/9601\n",
      "2017-10-12 16:24:06,766 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:07,194 : INFO : topic #45 (0.010): 0.011*\"tonne\" + 0.009*\"price\" + 0.009*\"opec\" + 0.008*\"export\" + 0.007*\"oil\" + 0.007*\"output\" + 0.006*\"trade\" + 0.006*\"he\" + 0.006*\"official\" + 0.006*\"import\"\n",
      "2017-10-12 16:24:07,195 : INFO : topic #59 (0.010): 0.025*\"preferred\" + 0.017*\"relief\" + 0.014*\"saudi\" + 0.011*\"wind\" + 0.011*\"share\" + 0.011*\"life\" + 0.011*\"floating\" + 0.010*\"halted\" + 0.010*\"australia\" + 0.010*\"release\"\n",
      "2017-10-12 16:24:07,197 : INFO : topic #9 (0.010): 0.021*\"agreement\" + 0.021*\"buy\" + 0.019*\"asset\" + 0.018*\"subject\" + 0.017*\"inc\" + 0.016*\"undisclosed\" + 0.015*\"unit\" + 0.015*\"acquisition\" + 0.014*\"approval\" + 0.014*\"venture\"\n",
      "2017-10-12 16:24:07,198 : INFO : topic #58 (0.010): 0.016*\"bankamerica\" + 0.010*\"bank\" + 0.009*\"tokyo\" + 0.007*\"rate\" + 0.007*\"bond\" + 0.006*\"provides\" + 0.006*\"competitor\" + 0.006*\"creditor\" + 0.006*\"pct\" + 0.006*\"issue\"\n",
      "2017-10-12 16:24:07,199 : INFO : topic #5 (0.010): 0.031*\"gnp\" + 0.023*\"3rd\" + 0.015*\"alberta\" + 0.014*\"pct\" + 0.013*\"unchanged\" + 0.013*\"rise\" + 0.011*\"realty\" + 0.011*\"central\" + 0.009*\"nine\" + 0.009*\"know\"\n",
      "2017-10-12 16:24:07,201 : INFO : topic diff=1.164674, rho=0.577350\n",
      "2017-10-12 16:24:07,407 : INFO : PROGRESS: pass 0, at document #8000/9601\n",
      "2017-10-12 16:24:13,439 : INFO : merging changes from 2000 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:13,765 : INFO : topic #19 (0.010): 0.084*\"saving\" + 0.029*\"association\" + 0.028*\"loan\" + 0.024*\"home\" + 0.023*\"certificate\" + 0.021*\"institutional\" + 0.018*\"federal\" + 0.017*\"deposit\" + 0.014*\"kansa\" + 0.014*\"bank\"\n",
      "2017-10-12 16:24:13,766 : INFO : topic #3 (0.010): 0.079*\"gencorp\" + 0.036*\"telephone\" + 0.034*\"telecommunication\" + 0.033*\"communication\" + 0.025*\"line\" + 0.022*\"los\" + 0.022*\"angeles\" + 0.019*\"borrowing\" + 0.019*\"credit\" + 0.018*\"john\"\n",
      "2017-10-12 16:24:13,767 : INFO : topic #60 (0.010): 0.023*\"placement\" + 0.017*\"participation\" + 0.013*\"share\" + 0.013*\"ag\" + 0.013*\"car\" + 0.011*\"volkswagen\" + 0.011*\"consecutive\" + 0.010*\"guilder\" + 0.010*\"florida\" + 0.009*\"v\"\n",
      "2017-10-12 16:24:13,768 : INFO : topic #69 (0.010): 0.030*\"medical\" + 0.018*\"freight\" + 0.017*\"offering\" + 0.014*\"license\" + 0.013*\"att\" + 0.012*\"file\" + 0.012*\"share\" + 0.010*\"shelf\" + 0.009*\"underwriter\" + 0.009*\"exclusive\"\n",
      "2017-10-12 16:24:13,769 : INFO : topic #73 (0.010): 0.035*\"seaman\" + 0.030*\"painewebber\" + 0.028*\"bankruptcy\" + 0.024*\"furniture\" + 0.024*\"reorganization\" + 0.020*\"copper\" + 0.017*\"basic\" + 0.016*\"outside\" + 0.014*\"generation\" + 0.013*\"court\"\n",
      "2017-10-12 16:24:13,771 : INFO : topic diff=1.326013, rho=0.500000\n",
      "2017-10-12 16:24:26,316 : INFO : -13.694 per-word bound, 13256.3 perplexity estimate based on a held-out corpus of 1601 documents with 9081 words\n",
      "2017-10-12 16:24:26,318 : INFO : PROGRESS: pass 0, at document #9601/9601\n",
      "2017-10-12 16:24:31,009 : INFO : merging changes from 1601 documents into a model of 9601 documents\n",
      "2017-10-12 16:24:31,288 : INFO : topic #50 (0.010): 0.047*\"budget\" + 0.031*\"rating\" + 0.023*\"standard\" + 0.021*\"deficit\" + 0.021*\"aa\" + 0.017*\"lowered\" + 0.016*\"tax\" + 0.015*\"debt\" + 0.015*\"callable\" + 0.014*\"upgraded\"\n",
      "2017-10-12 16:24:31,290 : INFO : topic #30 (0.010): 0.078*\"cyclops\" + 0.047*\"dixons\" + 0.026*\"canada\" + 0.026*\"conversion\" + 0.019*\"canadian\" + 0.019*\"contingent\" + 0.018*\"zero\" + 0.017*\"king\" + 0.017*\"supply\" + 0.016*\"offer\"\n",
      "2017-10-12 16:24:31,291 : INFO : topic #60 (0.010): 0.026*\"placement\" + 0.018*\"ag\" + 0.016*\"participation\" + 0.015*\"fda\" + 0.014*\"facility\" + 0.013*\"volkswagen\" + 0.013*\"car\" + 0.012*\"florida\" + 0.011*\"share\" + 0.011*\"model\"\n",
      "2017-10-12 16:24:31,292 : INFO : topic #54 (0.010): 0.030*\"registration\" + 0.029*\"filed\" + 0.028*\"offering\" + 0.023*\"file\" + 0.022*\"security\" + 0.017*\"covering\" + 0.016*\"california\" + 0.016*\"statement\" + 0.015*\"incurred\" + 0.014*\"county\"\n",
      "2017-10-12 16:24:31,293 : INFO : topic #86 (0.010): 0.046*\"bill\" + 0.035*\"acre\" + 0.031*\"danish\" + 0.029*\"usda\" + 0.028*\"qualified\" + 0.024*\"crop\" + 0.021*\"chase\" + 0.018*\"farmer\" + 0.018*\"program\" + 0.017*\"manhattan\"\n",
      "2017-10-12 16:24:31,295 : INFO : topic diff=1.387438, rho=0.447214\n"
     ]
    }
   ],
   "source": [
    "# extract 4 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations\n",
    "lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4241091686516862\t Topic: 0.019*\"philippine\" + 0.017*\"trade\" + 0.016*\"cocoa\" + 0.013*\"bill\" + 0.011*\"taiwan\" + 0.011*\"tariff\" + 0.010*\"indonesia\" + 0.010*\"bank\" + 0.008*\"baldrige\" + 0.008*\"senate\"\n",
      "Score: 0.24113685576355734\t Topic: 0.009*\"tonne\" + 0.009*\"export\" + 0.008*\"he\" + 0.007*\"trade\" + 0.007*\"economic\" + 0.007*\"price\" + 0.006*\"import\" + 0.006*\"oil\" + 0.006*\"minister\" + 0.006*\"country\"\n",
      "Score: 0.11356546789795555\t Topic: 0.060*\"tonne\" + 0.022*\"exporter\" + 0.022*\"prudential\" + 0.021*\"wheat\" + 0.020*\"shipment\" + 0.019*\"park\" + 0.019*\"palm\" + 0.017*\"corn\" + 0.016*\"vegetable\" + 0.016*\"grain\"\n",
      "Score: 0.04811582951565254\t Topic: 0.017*\"moody\" + 0.017*\"shell\" + 0.016*\"oil\" + 0.015*\"crude\" + 0.015*\"brown\" + 0.015*\"coca\" + 0.015*\"cited\" + 0.014*\"cola\" + 0.012*\"downgrade\" + 0.011*\"bbl\"\n",
      "Score: 0.04196684828242752\t Topic: 0.020*\"yen\" + 0.017*\"latin\" + 0.017*\"dollar\" + 0.017*\"ecuador\" + 0.017*\"rio\" + 0.014*\"rate\" + 0.013*\"economy\" + 0.013*\"cooperate\" + 0.012*\"speaking\" + 0.012*\"nation\"\n",
      "Score: 0.035413358612613896\t Topic: 0.023*\"agreement\" + 0.022*\"inc\" + 0.020*\"unit\" + 0.019*\"acquisition\" + 0.019*\"undisclosed\" + 0.018*\"buy\" + 0.016*\"merger\" + 0.016*\"subsidiary\" + 0.016*\"approval\" + 0.014*\"sell\"\n"
     ]
    }
   ],
   "source": [
    "# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.\n",
    "for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_dataset = pickle.load( open( \"../data/LSA_Classification/data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n"
     ]
    }
   ],
   "source": [
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [lda[c] for c in corpus_tfidf]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors_matrix = gensim.matutils.corpus2dense(vectors, num_terms=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 100)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.transpose(vectors_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors_matrix = np.transpose(vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4367 / 4858) correct - 89.89%\n",
      "  done in 0.492sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(vectors_matrix[:4743], y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(vectors_matrix[4743:])\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以说使用lda后。反而效果不好了呢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
